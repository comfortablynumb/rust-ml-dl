//! # Diffusion Models: Denoising Diffusion Probabilistic Models
//!
//! This example explains Diffusion Models, the state-of-the-art generative models
//! that power Stable Diffusion, DALL-E 2, Midjourney, and Imagen.
//!
//! ## The Core Idea
//!
//! **Generate by learning to denoise:**
//!
//! ```
//! Traditional generative models:
//! Noise z ‚Üí Generator ‚Üí Image x
//!
//! Diffusion models:
//! Image x ‚Üí Gradually add noise ‚Üí Pure noise z
//!          ‚Üê Learn to reverse (denoise) ‚Üê
//!
//! Generation: Pure noise ‚Üí Denoise step by step ‚Üí Clean image
//! ```
//!
//! ### Intuitive Example
//!
//! ```
//! Start with photo of a cat
//! Step 1: Add tiny noise ‚Üí slightly blurry cat
//! Step 2: Add more noise ‚Üí blurrier cat
//! ...
//! Step 1000: Pure noise (no cat visible)
//!
//! Training: Learn to reverse each step
//! ‚Ä¢ Given "blurrier cat", predict "less blurry cat"
//! ‚Ä¢ Train neural network to denoise
//!
//! Generation:
//! Start: Pure random noise
//! Step 999: Denoise ‚Üí Very blurry image
//! Step 998: Denoise ‚Üí Less blurry
//! ...
//! Step 0: Denoise ‚Üí Sharp, realistic cat!
//! ```
//!
//! ## Two Processes
//!
//! ### Forward Process (Diffusion)
//!
//! **Add noise gradually:**
//!
//! ```
//! q(x_t | x_{t-1}) = N(x_t; ‚àö(1-Œ≤_t) x_{t-1}, Œ≤_t I)
//!
//! In words:
//! ‚Ä¢ Start with clean image x_0
//! ‚Ä¢ At each step t, add Gaussian noise
//! ‚Ä¢ Œ≤_t: noise schedule (how much noise to add)
//! ‚Ä¢ After T steps (e.g., T=1000): x_T ‚âà pure noise
//!
//! Example:
//! x_0: [original cat image]
//! x_1: [cat + tiny noise]
//! x_2: [cat + more noise]
//! ...
//! x_1000: [pure Gaussian noise]
//! ```
//!
//! ### Nice Property: Closed Form
//!
//! ```
//! Can jump directly to any timestep t:
//!
//! q(x_t | x_0) = N(x_t; ‚àö·æ±_t x_0, (1-·æ±_t) I)
//!
//! Where:
//! Œ±_t = 1 - Œ≤_t
//! ·æ±_t = ‚àè_{s=1}^t Œ±_s
//!
//! In code:
//! x_t = ‚àö·æ±_t ¬∑ x_0 + ‚àö(1-·æ±_t) ¬∑ Œµ
//! where Œµ ~ N(0,1)
//!
//! No need to apply noise 1000 times!
//! Can directly sample x_t from x_0
//! ```
//!
//! ### Reverse Process (Denoising)
//!
//! **Learn to remove noise:**
//!
//! ```
//! p_Œ∏(x_{t-1} | x_t) = N(x_{t-1}; Œº_Œ∏(x_t, t), Œ£_Œ∏(x_t, t))
//!
//! Train neural network to predict:
//! ‚Ä¢ Given noisy image x_t and timestep t
//! ‚Ä¢ Predict slightly less noisy x_{t-1}
//!
//! Network Œº_Œ∏(x_t, t):
//! ‚Ä¢ Input: Noisy image x_t + timestep t
//! ‚Ä¢ Output: Denoised image x_{t-1}
//!
//! Often parameterized as predicting noise Œµ_Œ∏:
//! ‚Ä¢ Instead of predicting x_{t-1}
//! ‚Ä¢ Predict the noise Œµ that was added
//! ‚Ä¢ More stable training!
//! ```
//!
//! ## Training Diffusion Models
//!
//! ### Training Objective
//!
//! ```
//! Simplified loss (DDPM):
//!
//! L_simple = ùîº_t,x_0,Œµ [||Œµ - Œµ_Œ∏(x_t, t)||¬≤]
//!
//! Algorithm:
//! 1. Sample image x_0 from dataset
//! 2. Sample timestep t ~ Uniform(1, T)
//! 3. Sample noise Œµ ~ N(0, I)
//! 4. Compute noisy image: x_t = ‚àö·æ±_t x_0 + ‚àö(1-·æ±_t) Œµ
//! 5. Predict noise: Œµ_pred = Œµ_Œ∏(x_t, t)
//! 6. Compute loss: ||Œµ - Œµ_pred||¬≤
//! 7. Backpropagate
//!
//! Interpretation:
//! Train network to predict what noise was added!
//! ```
//!
//! ### Noise Schedule
//!
//! ```
//! Œ≤_t: How much noise to add at each step
//!
//! Common schedules:
//!
//! 1. Linear:
//! Œ≤_1 = 0.0001
//! Œ≤_T = 0.02
//! Œ≤_t = linear interpolation
//!
//! 2. Cosine (better):
//! More noise early, less at end
//! Smoother transition
//!
//! Typical: T = 1000 steps
//! ```
//!
//! ## Generation (Sampling)
//!
//! ### Sampling Algorithm (DDPM)
//!
//! ```
//! # Start with pure noise
//! x_T ~ N(0, I)
//!
//! # Denoise step by step
//! for t in [T, T-1, ..., 1]:
//!     # Predict noise
//!     Œµ_pred = Œµ_Œ∏(x_t, t)
//!
//!     # Compute less noisy image
//!     x_{t-1} = (1/‚àöŒ±_t) ¬∑ (x_t - ((1-Œ±_t)/‚àö(1-·æ±_t)) ¬∑ Œµ_pred)
//!
//!     # Add noise (except last step)
//!     if t > 1:
//!         z ~ N(0, I)
//!         x_{t-1} += œÉ_t ¬∑ z
//!
//! return x_0  # Final denoised image
//!
//! Time: T forward passes (slow but high quality!)
//! ```
//!
//! ### Faster Sampling (DDIM)
//!
//! ```
//! Denoising Diffusion Implicit Models (DDIM):
//! ‚Ä¢ Skip steps! Sample at t = [1000, 900, 800, ...]
//! ‚Ä¢ 50 steps instead of 1000
//! ‚Ä¢ 20√ó faster!
//! ‚Ä¢ Slight quality degradation
//!
//! Deterministic option:
//! ‚Ä¢ No added noise during sampling
//! ‚Ä¢ Same noise ‚Üí same image (reproducible)
//! ```
//!
//! ## Network Architecture
//!
//! ### U-Net with Time Embedding
//!
//! ```
//! Input: Noisy image x_t (e.g., 256√ó256√ó3)
//!        Timestep t (embedded as sinusoidal encoding)
//!
//! Architecture:
//!
//!     x_t (256√ó256)
//!        ‚Üì
//!   Encoder (downsampling)
//!     128√ó128 ‚Üí 64√ó64 ‚Üí 32√ó32
//!        ‚Üì
//!   Bottleneck (32√ó32) + Time embedding
//!        ‚Üì
//!   Decoder (upsampling)
//!     64√ó64 ‚Üí 128√ó128 ‚Üí 256√ó256
//!        ‚Üì
//!   Output: Predicted noise Œµ (256√ó256)
//!
//! Time embedding:
//! ‚Ä¢ Convert t to high-dim vector (like positional encoding)
//! ‚Ä¢ Add to features at each layer
//! ‚Ä¢ Network learns different behavior per timestep
//! ```
//!
//! ### Why U-Net?
//!
//! ```
//! ‚Ä¢ Skip connections preserve details
//! ‚Ä¢ Multi-scale features
//! ‚Ä¢ Proven in image-to-image tasks
//! ‚Ä¢ Same architecture as image segmentation
//! ```
//!
//! ## Conditional Generation
//!
//! ### Class-Conditional
//!
//! ```
//! Generate specific class (e.g., "dog"):
//!
//! Œµ_Œ∏(x_t, t, c)  ‚Üê Add class label c
//!
//! Implementation:
//! ‚Ä¢ Embed class label
//! ‚Ä¢ Add to time embedding
//! ‚Ä¢ Network conditions on class
//!
//! Generation:
//! Choose class ‚Üí Generate that class
//! ```
//!
//! ### Text-Conditional (Stable Diffusion, DALL-E)
//!
//! ```
//! Generate from text prompt: "A cat wearing a hat"
//!
//! Architecture:
//! 1. Text encoder (CLIP, T5):
//!    "A cat..." ‚Üí text embedding
//!
//! 2. Cross-attention in U-Net:
//!    Image features attend to text features
//!
//! 3. Classifier-free guidance:
//!    Balance conditional and unconditional
//!
//! Œµ_total = Œµ_uncond + w ¬∑ (Œµ_cond - Œµ_uncond)
//! where w = guidance scale (1.0-15.0)
//!
//! Higher w: Stronger text adherence, less diversity
//! Lower w: More diversity, weaker text match
//! ```
//!
//! ## Classifier-Free Guidance
//!
//! **Key technique for controllable generation:**
//!
//! ```
//! Problem: How to make generation follow text strongly?
//!
//! Solution: Train one model for both:
//! ‚Ä¢ Conditional: p(x|text)
//! ‚Ä¢ Unconditional: p(x)  (random dropout text 10% of time)
//!
//! At generation:
//! noise_pred = noise_uncond + guidance_scale √ó (noise_cond - noise_uncond)
//!
//! Guidance scale:
//! ‚Ä¢ 1.0: Unconditional (ignore text)
//! ‚Ä¢ 7.5: Default for Stable Diffusion
//! ‚Ä¢ 15.0: Very strong text adherence (may be less realistic)
//! ```
//!
//! ## Latent Diffusion (Stable Diffusion)
//!
//! **Diffusion in compressed space:**
//!
//! ```
//! Problem: Diffusion in pixel space is slow
//! ‚Ä¢ 512√ó512 image = 786,432 dimensions
//! ‚Ä¢ 1000 steps √ó huge U-Net = very slow
//!
//! Solution: Work in latent space
//!
//! 1. VAE encoder: Image (512√ó512) ‚Üí Latent (64√ó64)
//!    ‚Üì (8√ó compression)
//!
//! 2. Diffusion in latent space (64√ó64)
//!    ‚Üì (8√ó smaller = 64√ó faster)
//!
//! 3. VAE decoder: Latent (64√ó64) ‚Üí Image (512√ó512)
//!
//! Result: 8√ó faster, similar quality!
//! ```
//!
//! ### Stable Diffusion Architecture
//!
//! ```
//! Components:
//!
//! 1. CLIP Text Encoder:
//!    "A cat..." ‚Üí text_embedding (77√ó768)
//!
//! 2. VAE Encoder:
//!    Image (512√ó512√ó3) ‚Üí latent (64√ó64√ó4)
//!
//! 3. U-Net (diffusion model):
//!    latent + text_embedding ‚Üí denoised_latent
//!    ‚Ä¢ Cross-attention layers
//!    ‚Ä¢ ResNet blocks
//!    ‚Ä¢ Self-attention
//!
//! 4. VAE Decoder:
//!    latent (64√ó64√ó4) ‚Üí Image (512√ó512√ó3)
//!
//! Generation:
//! Text ‚Üí CLIP ‚Üí Diffusion (50 steps) ‚Üí VAE ‚Üí Image
//! Time: ~3 seconds on GPU
//! ```
//!
//! ## Applications
//!
//! ### Text-to-Image
//!
//! ```
//! Stable Diffusion, DALL-E 2, Midjourney, Imagen
//!
//! "A photo of an astronaut riding a horse"
//!   ‚Üì
//! Photorealistic image
//!
//! Capabilities:
//! ‚Ä¢ Composition: "cat on table"
//! ‚Ä¢ Styles: "oil painting", "3D render"
//! ‚Ä¢ Attributes: "blue eyes", "wearing hat"
//! ‚Ä¢ Concepts: "in the style of Van Gogh"
//! ```
//!
//! ### Image Editing
//!
//! **Inpainting:**
//! ```
//! Mask part of image + text prompt
//! ‚Üí Fill in masked region coherently
//!
//! Example:
//! Image: photo of room
//! Mask: empty wall
//! Prompt: "oil painting of mountains"
//! Result: Painting appears on wall
//! ```
//!
//! **Image-to-Image:**
//! ```
//! Start from existing image (not pure noise)
//! Add noise to step t
//! Denoise with text guidance
//! ‚Üí Modified image
//!
//! Example:
//! Input: Sketch of a cat
//! Prompt: "Realistic cat photo"
//! Result: Photorealistic version of sketch
//! ```
//!
//! ### Super-Resolution
//!
//! ```
//! Low-res image ‚Üí Diffusion model ‚Üí High-res image
//!
//! Used in:
//! ‚Ä¢ Photography enhancement
//! ‚Ä¢ Old photo restoration
//! ‚Ä¢ Medical imaging
//! ```
//!
//! ### Video Generation
//!
//! ```
//! Extend to temporal dimension:
//! ‚Ä¢ 3D U-Net (spatial + temporal)
//! ‚Ä¢ Generate frame by frame
//! ‚Ä¢ Ensure temporal consistency
//!
//! Examples: Runway Gen-2, Pika, Sora (OpenAI)
//! ```
//!
//! ### Other Modalities
//!
//! ```
//! Audio: Generate music, speech
//! 3D: Generate 3D models
//! Molecules: Drug discovery
//! Protein: Predict protein structures
//! ```
//!
//! ## Why Diffusion Models Won
//!
//! ### vs GANs
//!
//! ```
//! Diffusion:
//! ‚úÖ Stable training (no mode collapse)
//! ‚úÖ Higher quality
//! ‚úÖ Better diversity
//! ‚úÖ Easier to scale
//! ‚ùå Slow generation (many steps)
//!
//! GAN:
//! ‚úÖ Fast generation (one pass)
//! ‚ùå Training instability
//! ‚ùå Mode collapse
//! ‚ùå Lower diversity
//!
//! 2022+: Diffusion is the winner for images
//! ```
//!
//! ### vs VAE
//!
//! ```
//! Diffusion:
//! ‚úÖ Sharp, high-quality outputs
//! ‚úÖ Better for complex data
//! ‚ùå Slow generation
//! ‚ùå Hard to get latent representation
//!
//! VAE:
//! ‚úÖ Fast generation
//! ‚úÖ Explicit latent space
//! ‚ùå Blurry outputs
//!
//! Hybrid: Stable Diffusion uses VAE + Diffusion!
//! ```
//!
//! ### vs Autoregressive (GPT-style)
//!
//! ```
//! Diffusion:
//! ‚úÖ Parallel denoising
//! ‚úÖ Better for images
//! ‚úÖ Continuous data
//!
//! Autoregressive:
//! ‚úÖ Better for discrete data (text)
//! ‚úÖ Exact likelihood
//! ‚ùå Sequential generation (slow for images)
//! ```
//!
//! ## Training Diffusion Models
//!
//! ### Data Requirements
//!
//! ```
//! High quality images: 10M - 1B+
//!
//! Examples:
//! ‚Ä¢ LAION-5B: 5 billion image-text pairs
//! ‚Ä¢ Used for Stable Diffusion
//! ‚Ä¢ Filtered for quality, safety
//!
//! Can train on smaller datasets:
//! ‚Ä¢ Few thousand images for specific domain
//! ‚Ä¢ Fine-tune from pretrained model
//! ```
//!
//! ### Computational Requirements
//!
//! ```
//! Training from scratch:
//! ‚Ä¢ 256√ó256 images: 100-500 GPU-days
//! ‚Ä¢ 512√ó512 images: 1000+ GPU-days
//! ‚Ä¢ Use A100 GPUs (80GB)
//!
//! Fine-tuning:
//! ‚Ä¢ 10-100 GPU-hours
//! ‚Ä¢ Consumer GPUs possible (RTX 3090)
//!
//! Inference:
//! ‚Ä¢ 512√ó512 image: 2-5 seconds (GPU)
//! ‚Ä¢ 50 diffusion steps
//! ‚Ä¢ Can optimize (distillation ‚Üí 1 step!)
//! ```
//!
//! ### Hyperparameters
//!
//! ```
//! Diffusion steps T: 1000 (training)
//! Noise schedule: Cosine
//! Optimizer: AdamW
//! Learning rate: 1e-4
//! Batch size: 64-256
//! Image size: 256, 512, or 1024
//!
//! Sampling steps: 20-50 (inference)
//! Guidance scale: 7.5 (text-to-image)
//! ```
//!
//! ## Advanced Techniques
//!
//! ### Cascaded Diffusion
//!
//! ```
//! Generate at increasing resolutions:
//!
//! Model 1: 64√ó64
//! Model 2: 64√ó64 ‚Üí 256√ó256 (super-resolution)
//! Model 3: 256√ó256 ‚Üí 1024√ó1024 (super-resolution)
//!
//! Used in: DALL-E 2, Imagen
//! Benefit: Higher resolution, better quality
//! ```
//!
//! ### Distillation
//!
//! ```
//! Problem: 50 steps is slow
//!
//! Solution: Train smaller model to mimic in 1-4 steps
//! ‚Ä¢ Student model learns to predict 50-step output
//! ‚Ä¢ 10-50√ó faster
//! ‚Ä¢ Slight quality loss
//!
//! Examples: Progressive Distillation, Consistency Models
//! ```
//!
//! ### ControlNet
//!
//! ```
//! Add spatial control to Stable Diffusion:
//!
//! Inputs:
//! ‚Ä¢ Text: "A photo of a cat"
//! ‚Ä¢ Control: Edge map, depth map, pose
//!
//! Output: Image matching both text AND control
//!
//! Uses:
//! ‚Ä¢ Precise composition
//! ‚Ä¢ Preserve structure
//! ‚Ä¢ Artistic control
//! ```
//!
//! ## Practical Tips
//!
//! ### Prompt Engineering
//!
//! ```
//! Bad: "cat"
//! Good: "A professional photograph of a fluffy cat, high detail, 8k"
//!
//! Tips:
//! ‚Ä¢ Be specific
//! ‚Ä¢ Mention style ("oil painting", "3D render")
//! ‚Ä¢ Add quality terms ("highly detailed", "8k")
//! ‚Ä¢ Use negative prompts (what to avoid)
//! ```
//!
//! ### Sampling Settings
//!
//! ```
//! Steps: 20-50
//! ‚Ä¢ 20: Fast, lower quality
//! ‚Ä¢ 50: Slower, better quality
//!
//! Guidance scale: 1-15
//! ‚Ä¢ 1: Diverse, may ignore text
//! ‚Ä¢ 7.5: Balanced (default)
//! ‚Ä¢ 15: Strong text match, less diverse
//!
//! Sampler: DDIM, DPM++, Euler
//! ‚Ä¢ Different noise schedules
//! ‚Ä¢ Subtle differences
//! ‚Ä¢ DPM++ often good
//! ```
//!
//! ## Historical Impact
//!
//! **2015:** Early diffusion work (Sohl-Dickstein)
//! - Theoretical foundation
//! - Impractical to train
//!
//! **2020:** DDPM (Denoising Diffusion Probabilistic Models)
//! - Ho et al., made diffusion practical
//! - Beat GANs on some benchmarks
//!
//! **2021:** Improved Diffusion (OpenAI)
//! - Classifier guidance
//! - Higher quality than GANs
//!
//! **2021:** GLIDE (OpenAI)
//! - Text-to-image with diffusion
//! - Photorealistic results
//!
//! **2022:** DALL-E 2 (OpenAI)
//! - Cascaded diffusion
//! - Amazing text-to-image
//! - Not open source
//!
//! **2022:** Stable Diffusion (Stability AI)
//! - Open source!
//! - Latent diffusion
//! - Consumer GPUs
//! - Democratized AI art
//!
//! **2022:** Imagen (Google)
//! - Text encoder: T5
//! - Cascaded models
//! - State-of-the-art quality
//!
//! **2023:** Midjourney v5
//! - Artistic generations
//! - Commercial success
//!
//! **2024:** Sora (OpenAI)
//! - Text-to-video
//! - 1-minute videos
//! - Photorealistic
//!
//! **Legacy:**
//! - Replaced GANs as #1 generative model
//! - Enabled AI art revolution
//! - Billion-dollar industry

fn main() {
    println!("=== Diffusion Models: State-of-the-Art Generation ===\n");

    println!("This example explains Diffusion Models, the technology behind");
    println!("Stable Diffusion, DALL-E 2, Midjourney, and Imagen.\n");

    println!("üìö Key Concepts Covered:");
    println!("  ‚Ä¢ Forward diffusion (gradual noising)");
    println!("  ‚Ä¢ Reverse process (denoising)");
    println!("  ‚Ä¢ Training objective (noise prediction)");
    println!("  ‚Ä¢ Classifier-free guidance");
    println!("  ‚Ä¢ Latent diffusion (Stable Diffusion)");
    println!("  ‚Ä¢ Text-to-image generation\n");

    println!("üéØ Why This Matters:");
    println!("  ‚Ä¢ Powers modern AI art (Stable Diffusion, DALL-E, Midjourney)");
    println!("  ‚Ä¢ Replaced GANs as best generative model");
    println!("  ‚Ä¢ Enabled text-to-image revolution");
    println!("  ‚Ä¢ State-of-the-art quality and diversity");
    println!("  ‚Ä¢ Foundation of modern creative AI\n");

    println!("See the source code documentation for comprehensive explanations!");
}
