# Efficient Transformers âš¡

**Making Transformers fast and scalable**: Reduce the O(nÂ²) attention bottleneck to O(n) with linear attention, Flash Attention, and sparse patterns.

## Overview

Standard Transformers have a critical problem: **quadratic complexity** in sequence length. This makes them impractical for:
- Long documents (>512 tokens)
- High-resolution images (>224Ã—224)
- Audio/video (very long sequences)
- Real-time applications

**The Problem:**
```
Standard attention: O(nÂ²) memory and compute
For n=1024: ~1M operations
For n=4096: ~16M operations (16Ã— worse!)
For n=16384: ~256M operations (256Ã— worse!)
```

Efficient Transformers solve this with clever approximations and algorithmic improvements.

## The Attention Bottleneck

### Standard Self-Attention (O(nÂ²))

```
Q, K, V = Linear projections (n Ã— d)

Attention scores: S = QK^T / âˆšd_k     # O(nÂ²) memory!
Softmax: A = softmax(S)                # O(nÂ²)
Output: O = AV                         # O(nÂ²)
```

**Why O(nÂ²)?**
- QK^T creates nÃ—n attention matrix
- Every token attends to every other token
- For n=2048: 4M attention scores!

**Memory Bottleneck:**
- GPT-3 (2048 tokens): 4M floats per attention head
- 96 heads: 384M floats = 1.5GB per layer
- 96 layers: 144GB just for attention!

## Efficient Attention Approaches

### 1. Linear Attention âš¡

**Paper:** Katharopoulos et al. (2020) - "Transformers are RNNs"

**Key Insight:** Reorder the attention computation!

**Standard:**
```
Attention(Q, K, V) = softmax(QK^T) V
                      â†‘
                    O(nÂ²)
```

**Linear Attention:**
```
Attention(Q, K, V) = Ï†(Q) (Ï†(K)^T V)
                            â†‘
                          O(n)
```

**The Trick:**
```
Standard: Compute QK^T first (nÃ—n matrix), then multiply by V
Linear: Compute K^TV first (dÃ—d matrix), then multiply by Q

Matrix dimensions:
  Q: n Ã— d
  K: n Ã— d
  V: n Ã— d

Standard: (nÃ—d @ dÃ—n) @ nÃ—d = nÃ—n @ nÃ—d = O(nÂ²d)
Linear: nÃ—d @ (dÃ—n @ nÃ—d) = nÃ—d @ dÃ—d = O(ndÂ²)

When d << n: Linear is much faster!
```

**Feature Map Ï†:**
```
Replace softmax with kernel feature map:
  softmax(q^Tk) â‰ˆ Ï†(q)^T Ï†(k)

Common choices:
  - ELU + 1: Ï†(x) = elu(x) + 1
  - ReLU: Ï†(x) = relu(x)
  - Random Fourier Features
```

**Benefits:**
- O(n) complexity instead of O(nÂ²)
- Can be computed recurrently (like RNN!)
- Constant memory
- 10-100Ã— faster for long sequences

**Drawbacks:**
- Approximation (not exact attention)
- Slightly lower accuracy (~1-2%)
- Harder to train

### 2. Flash Attention ðŸ”¥

**Paper:** Dao et al. (2022) - "FlashAttention: Fast and Memory-Efficient Exact Attention"

**Breakthrough:** Exact attention with O(n) memory through clever I/O optimization!

**The Problem:**
- GPUs have fast HBM (High Bandwidth Memory) but limited size
- Transferring nÃ—n attention matrix to/from memory is the bottleneck
- Standard attention: Many read/write operations

**FlashAttention Algorithm:**
```
1. Divide Q, K, V into blocks (fit in SRAM)
2. Compute attention block-by-block
3. Use online softmax (streaming algorithm)
4. Never materialize full nÃ—n matrix
5. Fuse operations (minimize memory transfers)
```

**Key Innovation: Tiled Computation**
```
Standard: Compute full QK^T, then softmax, then multiply V
Flash: For each tile:
  - Load Q_block, K_block from HBM to SRAM
  - Compute local attention scores
  - Apply softmax incrementally
  - Update output
  - Never store full attention matrix!
```

**Online Softmax:**
```
Standard softmax: Need full row for normalization
Online softmax: Update running max and sum

for each token:
  Update max_score
  Update exp_sum (with rescaling)
  Output = weighted combination

Final: Normalize once at end
```

**Benefits:**
- **2-4Ã— faster** than standard attention
- **5-20Ã— less memory** (no nÃ—n matrix!)
- **Exact** (same output as standard attention)
- Enables training with 64K token sequences
- **Training speedup**: 15% faster BERT, 3Ã— faster GPT-2

**Drawbacks:**
- Implementation complexity
- Requires custom CUDA kernels
- Not all frameworks support it yet

**Impact:**
- Used in GPT-4, Llama 2, PaLM
- Standard for training large models
- Enabled long-context models (32K, 100K tokens)

### 3. Sparse Attention Patterns ðŸ•¸ï¸

**Idea:** Not all tokens need to attend to all other tokens - use structured sparsity!

#### 3a. Local Attention (Sliding Window)

```
Each token attends to k neighbors:
  Token i attends to [i-k/2, ..., i, ..., i+k/2]

Complexity: O(nk) where k << n
Example: k=128 gives 16Ã— speedup for n=2048
```

**Use case:** Language modeling (local context usually sufficient)

#### 3b. Strided Attention

```
Every k-th position attends globally
Others attend locally

Pattern:
  Token 0, k, 2k, ... â†’ Full attention (anchor tokens)
  Other tokens â†’ Local attention
```

**Example (Sparse Transformer):**
```
For n=2048, k=128:
  Tokens 0, 128, 256, ... â†’ Attend to all 2048
  Token 5 â†’ Attends to [0, 1, 2, ..., 10] (local)
```

#### 3c. Random Attention

```
Each token attends to:
  - Local neighbors (always)
  - Random sample of r tokens (for global info)

Complexity: O(n(k + r))
```

**Used in:** BigBird, Longformer

#### 3d. Block-Sparse Attention

```
Divide into blocks, use sparse block patterns:
  - Diagonal blocks (local)
  - Strided blocks (global)
  - Random blocks (mix)
```

**Used in:** OpenAI's Sparse Transformer

### 4. Approximate Attention

#### Performer (Random Features)

**Paper:** Choromanski et al. (2021)

**Idea:** Approximate softmax attention with random Fourier features

```
softmax(QK^T) â‰ˆ Ï†_random(Q) Ï†_random(K)^T

Where Ï† uses random projections
Complexity: O(ndÂ²r) where r = num random features
```

**Benefits:**
- Provable approximation guarantees
- Works well in practice
- Maintains performance

#### Linformer (Low-Rank Projection)

**Paper:** Wang et al. (2020)

**Idea:** Project key and value to lower dimension

```
K_projected = K @ E  (nÃ—d â†’ kÃ—d, k << n)
V_projected = V @ F

Attention with projected K, V: O(nk)
```

**Observation:** Attention matrix is often low-rank

### 5. Hierarchical Attention

**Idea:** Multi-scale attention

```
Level 1: Fine-grained (local, n tokens)
Level 2: Medium-grained (chunks, n/4 tokens)
Level 3: Coarse-grained (global, n/16 tokens)

Total complexity: O(n + n/4 + n/16) â‰ˆ O(n)
```

**Used in:** Longformer, BigBird

## Comparison

| Method | Complexity | Memory | Exact? | Speedup (n=4096) | Use Case |
|--------|-----------|--------|--------|------------------|----------|
| Standard | O(nÂ²) | O(nÂ²) | âœ… Yes | 1Ã— (baseline) | n < 512 |
| Linear Attention | O(n) | O(1) | âŒ Approx | 10-50Ã— | Long sequences |
| Flash Attention | O(nÂ²)* | O(n) | âœ… Yes | 2-4Ã— | All cases |
| Sparse (Local) | O(nk) | O(nk) | âŒ No | 8-16Ã— | Language |
| Performer | O(n) | O(n) | âŒ Approx | 5-10Ã— | Long sequences |
| Linformer | O(nk) | O(nk) | âŒ Approx | 4-8Ã— | Fixed length |

*Same complexity but optimized I/O

## Real-World Applications

### Long Document Understanding

```
Standard Transformer: 512 tokens max (BERT)
Efficient Transformers: 4K-128K tokens

Applications:
  - Legal document analysis
  - Scientific paper understanding
  - Book-length context
```

**Models:**
- Longformer: 4K tokens (sparse attention)
- BigBird: 4K tokens (sparse patterns)
- LongT5: 16K tokens (local + global)

### High-Resolution Vision

```
Standard ViT: 224Ã—224 = 196 patches
Efficient ViT: 512Ã—512 = 1024 patches

Complexity:
  Standard: 196Â² = 38K
  Efficient: 1024Â² = 1M â†’ Use linear attention
```

### Long-Form Generation

```
GPT-3: 2048 tokens
GPT-4: 8K-32K tokens (likely Flash Attention)
Claude: 100K tokens

Use case: Long-form writing, code generation
```

### Real-Time Applications

```
Speech recognition: 10K+ audio frames
Video understanding: 1K+ frames
Real-time chat: Fast inference required

Solution: Linear attention for O(n) inference
```

## Modern Models Using Efficient Attention

| Model | Technique | Context Length | Year |
|-------|-----------|----------------|------|
| Longformer | Sparse (local+global) | 4K | 2020 |
| BigBird | Sparse (random+window+global) | 4K | 2020 |
| Performer | Random features | 64K | 2021 |
| Linformer | Low-rank projection | 4K | 2020 |
| GPT-4 | Flash Attention (rumored) | 8K-32K | 2023 |
| Llama 2 | Flash Attention | 4K | 2023 |
| Claude 2 | Unknown (likely Flash) | 100K | 2023 |

## Implementation Challenges

### Linear Attention
```python
# Challenge: Feature map choice affects quality
Ï†(x) = ?  # ELU+1, ReLU, RFF?

# Training stability
# Need careful initialization
```

### Flash Attention
```python
# Requires custom CUDA kernels
# Complex tiling logic
# Framework-specific

# Rust: Would need GPU programming
```

### Sparse Attention
```python
# Efficient sparse matrix ops
# Pattern definition
# Indexing complexity
```

## Best Practices

### When to Use What?

**Short sequences (n < 512):**
- Use standard attention (fast enough)
- Flash Attention for training speedup

**Medium sequences (512 < n < 4K):**
- Flash Attention (exact, fast)
- Local + Sparse for even more speed

**Long sequences (n > 4K):**
- Linear attention for inference
- Sparse patterns (Longformer-style)
- Flash Attention for training

**Memory-constrained:**
- Flash Attention (5-20Ã— less memory)
- Linear attention (constant memory)

**Latency-critical:**
- Linear attention (O(n) scales linearly)
- Local attention with small window

## Key Takeaways

1. **Flash Attention is the default** for training (exact, 2-4Ã— faster)
2. **Linear attention** enables 100K+ token context
3. **Sparse patterns** balance speed and quality
4. **Complexity matters**: nÂ² â†’ n is 100Ã— speedup for n=100
5. **Different tasks need different patterns** (language vs vision)

## Running the Example

```bash
cargo run --package efficient-transformers
```

Demonstrates:
- Complexity comparison (O(nÂ²) vs O(n))
- Linear attention simulation
- Sparse attention patterns
- Memory usage comparison

## References

- **Linear Attention:** Katharopoulos et al. (2020) - "Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention"
- **Flash Attention:** Dao et al. (2022) - "FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness"
- **Sparse Transformer:** Child et al. (2019) - "Generating Long Sequences with Sparse Transformers"
- **Longformer:** Beltagy et al. (2020) - "Longformer: The Long-Document Transformer"
- **Performer:** Choromanski et al. (2021) - "Rethinking Attention with Performers"
- **Linformer:** Wang et al. (2020) - "Linformer: Self-Attention with Linear Complexity"

## Impact

Efficient Transformers enabled:
- âœ… **Long-context models** (GPT-4 32K, Claude 100K)
- âœ… **Training speedup** (Flash Attention is standard)
- âœ… **High-resolution vision** (efficient ViT)
- âœ… **Real-time applications** (faster inference)
- âœ… **Accessible AI** (lower memory requirements)

**Without efficient attention, long-context AI would be impractical!**
