//! # Normalization Techniques: The Secret to Training Deep Networks
//!
//! Normalization layers are critical for training deep neural networks effectively.
//! This example explains BatchNorm, LayerNorm, GroupNorm, and when to use each.
//!
//! ## The Problem: Internal Covariate Shift
//!
//! **Without normalization:**
//! ```
//! Layer 1: outputs range [0, 1]
//! Layer 2: outputs range [-100, 100]  ‚Üê Unstable!
//! Layer 3: outputs range [0.001, 0.01]
//! ...
//!
//! Problems:
//! ‚Ä¢ Gradient vanishing/exploding
//! ‚Ä¢ Slow convergence
//! ‚Ä¢ Sensitive to initialization
//! ‚Ä¢ Requires very small learning rates
//! ```
//!
//! **With normalization:**
//! ```
//! Every layer: outputs normalized to mean=0, std=1
//! ‚Ä¢ Stable gradients
//! ‚Ä¢ Faster convergence (10-100√ó speedup)
//! ‚Ä¢ Higher learning rates possible
//! ‚Ä¢ Less sensitive to initialization
//! ```
//!
//! ## Batch Normalization (BatchNorm, 2015)
//!
//! **The breakthrough that enabled very deep networks**
//!
//! ### How It Works
//!
//! ```
//! For each mini-batch during training:
//!
//! 1. Compute mean and variance across batch:
//!    Œº_B = (1/m) Œ£ x_i
//!    œÉ¬≤_B = (1/m) Œ£ (x_i - Œº_B)¬≤
//!
//! 2. Normalize:
//!    xÃÇ_i = (x_i - Œº_B) / ‚àö(œÉ¬≤_B + Œµ)
//!
//! 3. Scale and shift (learnable):
//!    y_i = Œ≥ xÃÇ_i + Œ≤
//!
//! Where:
//! ‚Ä¢ Œº_B, œÉ¬≤_B: Batch statistics
//! ‚Ä¢ Œµ: Small constant (1e-5) for numerical stability
//! ‚Ä¢ Œ≥, Œ≤: Learnable parameters (scale and shift)
//! ```
//!
//! ### Why Scale and Shift (Œ≥, Œ≤)?
//!
//! ```
//! Without Œ≥, Œ≤: Forced to mean=0, std=1
//! Problem: Limits model expressiveness!
//!
//! Example: Sigmoid activation
//! ‚Ä¢ Linear region near 0 (normalized input)
//! ‚Ä¢ Can't use saturated regions
//!
//! With Œ≥, Œ≤:
//! ‚Ä¢ Model can learn to undo normalization if needed
//! ‚Ä¢ Best of both worlds: stable training + expressiveness
//! ```
//!
//! ### BatchNorm for Different Tensor Shapes
//!
//! **Fully Connected Layers:**
//! ```
//! Input: (N, C)  ‚Üê Batch size N, features C
//! Normalize: Across N (batch dimension)
//! Parameters: 2C (Œ≥ and Œ≤ for each feature)
//!
//! Example:
//! Input: (32, 128)  ‚Üê 32 samples, 128 features
//! ‚Üí Compute Œº, œÉ for each of 128 features across 32 samples
//! ```
//!
//! **Convolutional Layers:**
//! ```
//! Input: (N, C, H, W)  ‚Üê Batch, Channels, Height, Width
//! Normalize: Across N, H, W (keep C separate)
//! Parameters: 2C (Œ≥ and Œ≤ for each channel)
//!
//! Example:
//! Input: (32, 64, 28, 28)  ‚Üê 32 images, 64 channels, 28√ó28
//! ‚Üí For each channel: compute Œº, œÉ across 32√ó28√ó28 values
//! ‚Üí 64 channels = 128 parameters (64 Œ≥ + 64 Œ≤)
//! ```
//!
//! ### Training vs Inference
//!
//! **Training:**
//! ```
//! Use batch statistics (Œº_B, œÉ¬≤_B)
//! Update running average:
//! Œº_running = momentum √ó Œº_running + (1-momentum) √ó Œº_B
//! œÉ¬≤_running = momentum √ó œÉ¬≤_running + (1-momentum) √ó œÉ¬≤_B
//!
//! Typical momentum: 0.9 or 0.99
//! ```
//!
//! **Inference:**
//! ```
//! Use running statistics (Œº_running, œÉ¬≤_running)
//! Why? No batch at inference!
//! ‚Ä¢ Single image ‚Üí can't compute batch statistics
//! ‚Ä¢ Need deterministic output
//!
//! y = Œ≥ √ó (x - Œº_running)/‚àö(œÉ¬≤_running + Œµ) + Œ≤
//! ```
//!
//! ### Benefits
//!
//! ```
//! ‚úÖ Faster training (10√ó fewer iterations)
//! ‚úÖ Higher learning rates (10-100√ó larger)
//! ‚úÖ Less sensitive to initialization
//! ‚úÖ Regularization effect (noise from batch statistics)
//! ‚úÖ Enables very deep networks (ResNet-152, etc.)
//! ```
//!
//! ### Limitations
//!
//! ```
//! ‚ùå Batch size dependency
//!    ‚Ä¢ Small batches ‚Üí noisy statistics
//!    ‚Ä¢ Batch size < 8: poor performance
//!    ‚Ä¢ Training/inference discrepancy
//!
//! ‚ùå Not suitable for:
//!    ‚Ä¢ RNNs (variable sequence lengths)
//!    ‚Ä¢ Online learning (single sample)
//!    ‚Ä¢ Distributed training (sync required)
//! ```
//!
//! ## Layer Normalization (LayerNorm, 2016)
//!
//! **Solution for sequence models and Transformers**
//!
//! ### How It Works
//!
//! ```
//! Normalize across features (not batch!):
//!
//! For each sample independently:
//! Œº = (1/C) Œ£ x_i
//! œÉ¬≤ = (1/C) Œ£ (x_i - Œº)¬≤
//! xÃÇ = (x - Œº) / ‚àö(œÉ¬≤ + Œµ)
//! y = Œ≥ xÃÇ + Œ≤
//!
//! Key difference: Normalize within each sample
//! ```
//!
//! ### LayerNorm for Different Shapes
//!
//! **Fully Connected:**
//! ```
//! Input: (N, C)
//! Normalize: Across C for each sample
//! Parameters: 2C
//!
//! Example:
//! Input: (32, 128)
//! ‚Üí Each of 32 samples normalized independently
//! ‚Üí Compute Œº, œÉ from 128 features
//! ```
//!
//! **Transformers:**
//! ```
//! Input: (N, L, D)  ‚Üê Batch, Length, Dimension
//! Normalize: Across D for each (N, L) position
//! Parameters: 2D
//!
//! Example:
//! Input: (32, 512, 768)  ‚Üê 32 seqs, 512 tokens, 768 dims
//! ‚Üí Normalize 768 dims for each of 32√ó512 tokens
//! ```
//!
//! ### LayerNorm vs BatchNorm
//!
//! ```
//! BatchNorm:
//! ‚Ä¢ Normalize across batch dimension
//! ‚Ä¢ Requires batch size > 1
//! ‚Ä¢ Different behavior train/inference
//! ‚Ä¢ Good for CNNs
//!
//! LayerNorm:
//! ‚Ä¢ Normalize across feature dimension
//! ‚Ä¢ Works with batch size = 1
//! ‚Ä¢ Same behavior train/inference
//! ‚Ä¢ Good for RNNs, Transformers
//! ```
//!
//! ### Why LayerNorm for Transformers?
//!
//! ```
//! Transformers have:
//! ‚Ä¢ Variable sequence lengths
//! ‚Ä¢ Batch size often 1 at inference
//! ‚Ä¢ Need stable behavior regardless of batch
//!
//! LayerNorm advantages:
//! ‚Ä¢ No batch dependency
//! ‚Ä¢ Deterministic (no running stats)
//! ‚Ä¢ Works with any sequence length
//! ‚Ä¢ Used in: BERT, GPT, T5, all Transformers
//! ```
//!
//! ## Group Normalization (GroupNorm, 2018)
//!
//! **Best of both worlds for computer vision**
//!
//! ### How It Works
//!
//! ```
//! Split channels into groups, normalize within each group:
//!
//! 1. Divide C channels into G groups
//! 2. Normalize within each group
//!
//! Input: (N, C, H, W)
//! Groups: G (typically 32)
//! Channels per group: C/G
//!
//! For each group:
//!   Normalize across (C/G, H, W) for each sample
//!
//! Example:
//! Input: (32, 64, 28, 28)
//! Groups: 32
//! ‚Üí 64/32 = 2 channels per group
//! ‚Üí Normalize (2, 28, 28) = 1568 values per group
//! ```
//!
//! ### Special Cases
//!
//! ```
//! G = 1:  Group Normalization = Layer Normalization
//! G = C:  Group Normalization = Instance Normalization
//! G = 32: Typical choice (good performance)
//! ```
//!
//! ### Benefits
//!
//! ```
//! ‚úÖ No batch dependency (like LayerNorm)
//! ‚úÖ Works with batch size = 1
//! ‚úÖ Better than LayerNorm for CNNs
//! ‚úÖ More stable than BatchNorm with small batches
//! ‚úÖ Good for:
//!    ‚Ä¢ Object detection (batch size 1-2)
//!    ‚Ä¢ Segmentation
//!    ‚Ä¢ Video (memory constrained)
//! ```
//!
//! ## Instance Normalization (InstanceNorm)
//!
//! **For style transfer and GANs**
//!
//! ### How It Works
//!
//! ```
//! Normalize each channel of each sample independently:
//!
//! Input: (N, C, H, W)
//! For each (n, c):
//!   Normalize across (H, W)
//!
//! Example:
//! Input: (32, 64, 28, 28)
//! ‚Üí 32√ó64 = 2048 independent normalizations
//! ‚Üí Each over 28√ó28 = 784 values
//! ```
//!
//! ### Use Cases
//!
//! ```
//! Style Transfer:
//! ‚Ä¢ Remove style information (color, texture)
//! ‚Ä¢ Keep content (structure)
//!
//! GANs:
//! ‚Ä¢ Stabilize training
//! ‚Ä¢ Used in StyleGAN, Pix2Pix
//! ```
//!
//! ## When to Use Which?
//!
//! ### Quick Decision Tree
//!
//! ```
//! Are you using CNNs?
//!   ‚îú‚îÄ Yes ‚Üí Large batch size (>8)?
//!   ‚îÇ         ‚îú‚îÄ Yes ‚Üí BatchNorm
//!   ‚îÇ         ‚îî‚îÄ No ‚Üí GroupNorm
//!   ‚îÇ
//!   ‚îî‚îÄ No ‚Üí Using Transformers/RNNs?
//!             ‚îî‚îÄ Yes ‚Üí LayerNorm
//!
//! Style transfer or GANs?
//!   ‚îî‚îÄ InstanceNorm
//! ```
//!
//! ### Detailed Comparison Table
//!
//! | Technique | Normalize Over | Batch Dependent | Best For |
//! |-----------|----------------|-----------------|----------|
//! | **BatchNorm** | N (batch), H, W | ‚úÖ Yes | CNNs, large batches |
//! | **LayerNorm** | C, H, W | ‚ùå No | Transformers, RNNs |
//! | **GroupNorm** | C/G, H, W | ‚ùå No | CNNs, small batches |
//! | **InstanceNorm** | H, W | ‚ùå No | Style transfer, GANs |
//!
//! ### Modern Recommendations (2024)
//!
//! ```
//! Computer Vision:
//! ‚Ä¢ ResNet, VGG, etc: BatchNorm
//! ‚Ä¢ Object detection (YOLO, DETR): GroupNorm
//! ‚Ä¢ Small batch training: GroupNorm
//! ‚Ä¢ Style transfer: InstanceNorm
//!
//! NLP/Transformers:
//! ‚Ä¢ BERT, GPT, T5: LayerNorm (Pre-LN or Post-LN)
//! ‚Ä¢ All modern Transformers: LayerNorm
//!
//! Hybrid:
//! ‚Ä¢ Vision Transformers (ViT): LayerNorm
//! ‚Ä¢ Convnext: LayerNorm (CNNs using LN!)
//! ```
//!
//! ## Implementation Tips
//!
//! ### Placement in Network
//!
//! ```
//! Pre-Normalization (Modern Transformers):
//! x = x + MLP(LayerNorm(x))
//!     ‚Üë
//! Normalize BEFORE sub-layer
//!
//! Benefits:
//! ‚Ä¢ More stable training
//! ‚Ä¢ Can train deeper (100+ layers)
//! ‚Ä¢ Used in: GPT-3, BERT (modern variants)
//!
//! Post-Normalization (Original):
//! x = LayerNorm(x + MLP(x))
//!                ‚Üë
//! Normalize AFTER residual
//!
//! Original Transformer used this, but Pre-LN is now standard
//! ```
//!
//! ### Training Considerations
//!
//! ```
//! Learning Rate:
//! ‚Ä¢ With normalization: 10-100√ó higher LR possible
//! ‚Ä¢ BatchNorm: Try 0.1 instead of 0.001
//! ‚Ä¢ LayerNorm: Less sensitive, start with 0.001
//!
//! Weight Initialization:
//! ‚Ä¢ Less critical with normalization
//! ‚Ä¢ But still use Xavier/He initialization
//!
//! Warmup:
//! ‚Ä¢ Still beneficial for large models
//! ‚Ä¢ Gradually increase LR over first few epochs
//! ```
//!
//! ### Common Mistakes
//!
//! ```
//! ‚ùå Using BatchNorm with batch size 1
//! ‚ùå Forgetting to switch to eval mode (BatchNorm)
//! ‚ùå Using BatchNorm for RNNs
//! ‚ùå Not updating running stats (BatchNorm)
//! ‚ùå Applying normalization to every layer (overkill)
//!
//! ‚úÖ Normalize after conv/linear, before activation
//! ‚úÖ Use eval() mode for inference
//! ‚úÖ Match normalization to architecture type
//! ‚úÖ Monitor running stats during training
//! ```
//!
//! ## Advanced Topics
//!
//! ### Synchronized BatchNorm
//!
//! ```
//! Problem: Distributed training with small local batches
//!
//! Solution: Sync statistics across GPUs
//! ‚Ä¢ Compute Œº, œÉ across all GPUs
//! ‚Ä¢ Requires communication overhead
//! ‚Ä¢ Used in large-scale training
//! ```
//!
//! ### Adaptive Normalization
//!
//! ```
//! AdaIN (Adaptive Instance Normalization):
//! ‚Ä¢ Used in style transfer
//! ‚Ä¢ Modulate Œ≥, Œ≤ from style input
//! ‚Ä¢ y = Œ≥_style √ó normalize(x) + Œ≤_style
//!
//! SPADE (Spatially-Adaptive Normalization):
//! ‚Ä¢ Used in image generation (GauGAN)
//! ‚Ä¢ Spatially-varying normalization
//! ```
//!
//! ### Normalization-Free Networks
//!
//! ```
//! Recent research: Train without normalization
//! ‚Ä¢ NFNets (Normalizer-Free Networks)
//! ‚Ä¢ Careful initialization + activation
//! ‚Ä¢ Match BatchNorm performance
//!
//! Benefits:
//! ‚Ä¢ Simpler architecture
//! ‚Ä¢ No batch dependency
//! ‚Ä¢ Faster inference
//!
//! Still experimental for most use cases
//! ```
//!
//! ## Historical Impact
//!
//! **2015: BatchNorm**
//! - Ioffe & Szegedy
//! - Enabled training of very deep networks
//! - ResNet (152 layers) became possible
//!
//! **2016: LayerNorm**
//! - Ba et al.
//! - Solved RNN training issues
//! - Foundation for Transformers
//!
//! **2017: Transformer**
//! - Uses LayerNorm exclusively
//! - Proved effectiveness beyond RNNs
//!
//! **2018: GroupNorm**
//! - Wu & He
//! - Better than BatchNorm for small batches
//! - Widely adopted in detection/segmentation
//!
//! **2019+: Pre-LN Transformers**
//! - Pre-normalization becomes standard
//! - Enables GPT-3 scale models
//!
//! **Legacy:**
//! - Normalization layers are now standard
//! - Every modern architecture uses some form
//! - Critical enabler of deep learning success

fn main() {
    println!("=== Normalization Techniques ===\n");

    println!("Critical techniques for training deep neural networks effectively.\n");

    println!("üìö Techniques Covered:");
    println!("  ‚Ä¢ BatchNorm: Normalize across batch (CNNs, large batches)");
    println!("  ‚Ä¢ LayerNorm: Normalize across features (Transformers, RNNs)");
    println!("  ‚Ä¢ GroupNorm: Split into groups (small batch CNNs)");
    println!("  ‚Ä¢ InstanceNorm: Per-sample normalization (style transfer)\n");

    println!("üéØ Key Benefits:");
    println!("  ‚Ä¢ 10-100√ó faster training");
    println!("  ‚Ä¢ Higher learning rates possible");
    println!("  ‚Ä¢ Enables very deep networks (ResNet-152, GPT-3)");
    println!("  ‚Ä¢ Less sensitive to initialization");
    println!("  ‚Ä¢ Regularization effect\n");

    println!("üí° When to Use:");
    println!("  ‚Ä¢ CNNs + large batch ‚Üí BatchNorm");
    println!("  ‚Ä¢ CNNs + small batch ‚Üí GroupNorm");
    println!("  ‚Ä¢ Transformers/RNNs ‚Üí LayerNorm");
    println!("  ‚Ä¢ Style transfer/GANs ‚Üí InstanceNorm\n");

    println!("See source code documentation for comprehensive explanations!");
}
