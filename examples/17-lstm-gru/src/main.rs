//! # LSTM & GRU: Long Short-Term Memory and Gated Recurrent Units
//!
//! This example explains LSTM and GRU, the two most important RNN variants that
//! solved the vanishing gradient problem and enabled effective long-term memory.
//!
//! ## The Problem with Vanilla RNNs
//!
//! **Vanishing Gradients:**
//! ```
//! Standard RNN: h_t = tanh(W_hh * h_{t-1} + W_xh * x_t)
//!
//! Problem:
//! â€¢ Gradients multiply through time: âˆ‚L/âˆ‚h_1 = âˆ‚L/âˆ‚h_T Â· âˆ‚h_T/âˆ‚h_{T-1} Â· ... Â· âˆ‚h_2/âˆ‚h_1
//! â€¢ tanh derivative â‰¤ 1, often much smaller
//! â€¢ After 10+ steps: gradient â†’ 0 (vanishing)
//! â€¢ Can't learn long-term dependencies!
//! ```
//!
//! **Example Failure:**
//! ```
//! "The cat, which was sitting on the mat that was placed near the window, was hungry"
//!                                                                          â†‘
//! Predict "was" - needs to remember "cat" from 15 words ago
//! Vanilla RNN: âŒ Forgets "cat"
//! LSTM/GRU: âœ… Remembers "cat"
//! ```
//!
//! ## LSTM: Long Short-Term Memory (1997)
//!
//! **Key Innovation:** Explicit memory cell with gates
//!
//! ### Architecture
//!
//! ```
//! LSTM has TWO states:
//! â€¢ Cell state (C_t): Long-term memory highway
//! â€¢ Hidden state (h_t): Short-term output
//!
//! Three gates control information flow:
//! 1. Forget gate (f_t): What to forget from memory
//! 2. Input gate (i_t): What new info to store
//! 3. Output gate (o_t): What to output
//! ```
//!
//! ### Mathematical Formulation
//!
//! ```
//! Input: x_t, previous hidden h_{t-1}, previous cell C_{t-1}
//!
//! 1. Forget gate (what to forget):
//!    f_t = Ïƒ(W_f Â· [h_{t-1}, x_t] + b_f)
//!
//! 2. Input gate (what to add):
//!    i_t = Ïƒ(W_i Â· [h_{t-1}, x_t] + b_i)
//!    CÌƒ_t = tanh(W_C Â· [h_{t-1}, x_t] + b_C)  â† candidate values
//!
//! 3. Update cell state (memory update):
//!    C_t = f_t âŠ™ C_{t-1} + i_t âŠ™ CÌƒ_t
//!         â†‘                â†‘
//!    forget old       add new
//!
//! 4. Output gate (what to output):
//!    o_t = Ïƒ(W_o Â· [h_{t-1}, x_t] + b_o)
//!    h_t = o_t âŠ™ tanh(C_t)
//!
//! Where:
//! â€¢ Ïƒ = sigmoid function (0 to 1, acts as gate)
//! â€¢ âŠ™ = element-wise multiplication
//! â€¢ tanh = hyperbolic tangent (-1 to 1)
//! ```
//!
//! ### Why LSTM Works
//!
//! **Gradient Highway:**
//! ```
//! âˆ‚C_t/âˆ‚C_{t-1} = f_t (just multiplication, no repeated squashing!)
//!
//! â€¢ Cell state has direct path: C_1 â†’ C_2 â†’ ... â†’ C_T
//! â€¢ Gradients flow back unchanged (if f_t â‰ˆ 1)
//! â€¢ Can remember info for 100+ timesteps!
//! ```
//!
//! **Intuitive Example:**
//! ```
//! Input sequence: "The cat ate the mouse. Later it was full."
//!                                                  â†‘
//!                                           What does "it" refer to?
//!
//! Timestep 1: "The"
//!   f_t â‰ˆ 0 (forget empty memory)
//!   i_t â‰ˆ 1 (store: article detected)
//!
//! Timestep 2: "cat"
//!   f_t â‰ˆ 0.3 (partially forget article)
//!   i_t â‰ˆ 1 (store: subject = "cat")
//!   C_t stores: "cat is subject" â† REMEMBERED
//!
//! Timestep 3-7: "ate the mouse."
//!   f_t â‰ˆ 0.9 (keep subject in memory!)
//!   i_t â‰ˆ 0.5 (add action info)
//!
//! Timestep 8-9: "Later it"
//!   o_t queries memory
//!   Outputs: "cat" (subject from memory!)
//! ```
//!
//! ## GRU: Gated Recurrent Unit (2014)
//!
//! **Motivation:** LSTM works great but has many parameters. Can we simplify?
//!
//! ### Key Simplifications
//!
//! ```
//! GRU changes from LSTM:
//! 1. Merges cell state and hidden state (only h_t)
//! 2. Two gates instead of three:
//!    â€¢ Reset gate (r_t): How much past to forget
//!    â€¢ Update gate (z_t): Balance between past and new
//! 3. Fewer parameters â†’ faster training
//! ```
//!
//! ### Mathematical Formulation
//!
//! ```
//! Input: x_t, previous hidden h_{t-1}
//!
//! 1. Update gate (how much to update):
//!    z_t = Ïƒ(W_z Â· [h_{t-1}, x_t] + b_z)
//!
//! 2. Reset gate (how much past to use):
//!    r_t = Ïƒ(W_r Â· [h_{t-1}, x_t] + b_r)
//!
//! 3. Candidate hidden state:
//!    hÌƒ_t = tanh(W_h Â· [r_t âŠ™ h_{t-1}, x_t] + b_h)
//!          â†‘
//!    reset gate modulates how much past to use
//!
//! 4. Final hidden state:
//!    h_t = (1 - z_t) âŠ™ h_{t-1} + z_t âŠ™ hÌƒ_t
//!          â†‘                      â†‘
//!      keep old              add new
//!
//! Intuition:
//! â€¢ z_t = 1: Completely replace with new info
//! â€¢ z_t = 0: Keep old hidden state
//! â€¢ z_t = 0.5: Mix 50-50
//! ```
//!
//! ## LSTM vs GRU Comparison
//!
//! | Aspect | LSTM | GRU |
//! |--------|------|-----|
//! | **Gates** | 3 (forget, input, output) | 2 (update, reset) |
//! | **States** | 2 (cell C_t, hidden h_t) | 1 (hidden h_t) |
//! | **Parameters** | ~4Ã— input size | ~3Ã— input size |
//! | **Speed** | Slower | ~25% faster |
//! | **Memory** | More | Less |
//! | **Performance** | Slightly better on complex tasks | Comparable on most tasks |
//! | **Use When** | Large datasets, complex patterns | Limited data, faster training |
//!
//! ## When to Use Which?
//!
//! **Use LSTM when:**
//! - Very long sequences (100+ timesteps)
//! - Complex temporal patterns
//! - Large dataset (can afford the parameters)
//! - Need maximum accuracy
//! - Examples: machine translation, speech recognition, video analysis
//!
//! **Use GRU when:**
//! - Moderate sequence length (10-50 timesteps)
//! - Limited training data
//! - Need faster training/inference
//! - Resource constraints (mobile, edge)
//! - Examples: text classification, simple time series, sentiment analysis
//!
//! **Use Vanilla RNN when:**
//! - Very short sequences (< 10 timesteps)
//! - Simple patterns
//! - Teaching/understanding basics
//!
//! **Use Transformer when:**
//! - Have lots of data and compute
//! - Need parallelization
//! - Very long sequences with complex dependencies
//! - State-of-the-art performance required
//!
//! ## Bidirectional LSTM/GRU
//!
//! **Idea:** Process sequence in both directions
//!
//! ```
//! Forward LSTM:  The cat ate  â†’  â†’  â†’
//! Backward LSTM:  â†  â†  â† ate cat The
//!
//! Concatenate: [h_forward, h_backward]
//!
//! Benefits:
//! â€¢ Sees full context (past + future)
//! â€¢ Better for classification, tagging
//! â€¢ Can't use for generation (no future!)
//!
//! Use cases:
//! âœ… Named entity recognition
//! âœ… Part-of-speech tagging
//! âœ… Sentiment classification
//! âŒ Text generation (no future available)
//! âŒ Real-time prediction
//! ```
//!
//! ## Stacked/Deep LSTM/GRU
//!
//! **Architecture:**
//! ```
//! Layer 3: LSTM â†’ LSTM â†’ LSTM (high-level features)
//!           â†‘      â†‘      â†‘
//! Layer 2: LSTM â†’ LSTM â†’ LSTM (mid-level features)
//!           â†‘      â†‘      â†‘
//! Layer 1: LSTM â†’ LSTM â†’ LSTM (low-level features)
//!           â†‘      â†‘      â†‘
//! Input:   "The"  "cat"  "ate"
//!
//! Each layer learns different abstraction level:
//! â€¢ Layer 1: Syntax, word patterns
//! â€¢ Layer 2: Phrases, local context
//! â€¢ Layer 3: Semantics, global meaning
//! ```
//!
//! **Guidelines:**
//! - 1 layer: Simple tasks (sentiment, basic classification)
//! - 2 layers: Most tasks (translation, NER, tagging)
//! - 3-4 layers: Complex tasks (question answering, abstractive summarization)
//! - 5+ layers: Usually not helpful (use Transformers instead)
//!
//! ## Training Tips
//!
//! ### 1. Gradient Clipping
//! ```
//! Problem: While vanishing is solved, exploding can still occur
//!
//! Solution:
//! if ||gradient|| > threshold:
//!     gradient = gradient * (threshold / ||gradient||)
//!
//! Typical threshold: 1.0 to 5.0
//! ```
//!
//! ### 2. Initialization
//! ```
//! â€¢ Forget gate bias: Initialize to 1 or 2
//!   Reason: Start by remembering everything, learn to forget
//!
//! â€¢ Other biases: Initialize to 0
//! â€¢ Weights: Xavier/Glorot initialization
//! ```
//!
//! ### 3. Dropout
//! ```
//! âœ… Apply dropout on:
//!    â€¢ Between LSTM layers (vertical dropout)
//!    â€¢ On input to LSTM
//!
//! âŒ Don't apply dropout:
//!    â€¢ On recurrent connections (causes forgetting)
//!
//! Typical rate: 0.2 - 0.5
//! ```
//!
//! ### 4. Learning Rate
//! ```
//! Start: 0.001 (Adam) or 0.01 (SGD)
//! Schedule: Reduce by 2-10Ã— when plateaus
//! Warmup: Helpful for large models
//! ```
//!
//! ### 5. Batch Size
//! ```
//! â€¢ Larger batch: More stable, faster training
//! â€¢ Smaller batch: Better generalization
//! â€¢ Typical: 32-128 for NLP, 64-256 for time series
//! ```
//!
//! ## Applications & Success Stories
//!
//! ### Natural Language Processing
//! ```
//! Machine Translation (pre-Transformer era):
//! â€¢ Google Neural Machine Translation (2016)
//! â€¢ 8-layer stacked LSTM
//! â€¢ Reduced translation errors by 60%
//!
//! Sentiment Analysis:
//! â€¢ IMDB review classification
//! â€¢ Bidirectional LSTM: 89% accuracy
//! â€¢ Simple CNN: 87% accuracy
//! ```
//!
//! ### Speech Recognition
//! ```
//! Google Voice Search:
//! â€¢ LSTM-based acoustic model
//! â€¢ 49% error reduction
//! â€¢ Now handles accents, noise
//! ```
//!
//! ### Time Series Forecasting
//! ```
//! Stock price prediction:
//! â€¢ GRU often outperforms LSTM
//! â€¢ Faster training on financial data
//! â€¢ Captures market momentum
//!
//! Weather forecasting:
//! â€¢ Stacked LSTM for multi-day prediction
//! â€¢ Learns seasonal patterns
//! ```
//!
//! ### Video Analysis
//! ```
//! Action recognition:
//! â€¢ CNN extracts frame features
//! â€¢ LSTM models temporal dynamics
//! â€¢ UCF101 dataset: 94% accuracy
//! ```
//!
//! ### Music Generation
//! ```
//! â€¢ LSTM learns note patterns
//! â€¢ Generates Bach-style chorales
//! â€¢ Remembers musical themes
//! ```
//!
//! ## Modern Context: LSTM/GRU vs Transformers
//!
//! **Transformers took over (2017+) because:**
//! ```
//! âœ… Parallelization: Process all tokens at once
//! âœ… Better long-range dependencies via attention
//! âœ… Scale better with data/compute
//!
//! Examples:
//! â€¢ BERT (2018): Replaced LSTM for NLP
//! â€¢ GPT series: Pure decoder transformers
//! â€¢ No recurrence needed!
//! ```
//!
//! **LSTM/GRU still useful for:**
//! ```
//! âœ… Online/streaming processing (can't wait for full sequence)
//! âœ… Limited memory (O(1) vs O(nÂ²) for Transformer)
//! âœ… Small datasets (fewer parameters to train)
//! âœ… Real-time applications (lower latency)
//! âœ… Time series forecasting (temporal inductive bias)
//! âœ… Audio processing (continuous streams)
//!
//! Examples:
//! â€¢ IoT sensor data analysis
//! â€¢ Real-time speech recognition
//! â€¢ Anomaly detection in streams
//! â€¢ Mobile applications
//! ```
//!
//! ## Variants and Extensions
//!
//! ### Peephole LSTM
//! ```
//! Enhancement: Gates can see cell state
//!
//! f_t = Ïƒ(W_f Â· [C_{t-1}, h_{t-1}, x_t] + b_f)
//!                â†‘
//!         Peephole connection
//!
//! Benefit: More precise timing control
//! Use: When exact timing matters (music, speech)
//! ```
//!
//! ### Coupled Input-Forget Gates
//! ```
//! Observation: Input and forget gates often opposite
//!
//! Standard:  f_t and i_t independent
//! Coupled:   i_t = 1 - f_t
//!
//! Benefit: 25% fewer parameters, similar performance
//! ```
//!
//! ### Layer Normalization
//! ```
//! Add normalization within LSTM:
//!
//! h_t = LayerNorm(LSTM(x_t, h_{t-1}))
//!
//! Benefits:
//! â€¢ Faster training
//! â€¢ Better generalization
//! â€¢ Allows higher learning rates
//! ```
//!
//! ## Implementation Considerations
//!
//! ### Computational Complexity
//! ```
//! LSTM forward pass:
//! â€¢ Memory: O(n Ã— h) where n=sequence length, h=hidden size
//! â€¢ Time: O(n Ã— hÂ²) (matrix multiplications)
//! â€¢ Compared to Transformer: O(nÂ² Ã— h)
//!
//! LSTM is faster for: n > h (long sequences, small hidden size)
//! Transformer is faster for: h > n (short sequences, large hidden size)
//! ```
//!
//! ### Batch Processing
//! ```
//! Challenge: Sequences have different lengths
//!
//! Solutions:
//! 1. Padding + masking:
//!    Seq1: [1, 2, 3, 0, 0]  â† padded
//!    Seq2: [4, 5, 6, 7, 8]  â† full
//!    Mask: [1, 1, 1, 0, 0]  â† ignore padded
//!
//! 2. Packing (more efficient):
//!    Pack sequences â†’ process â†’ unpack
//!    Avoids computing on padding
//! ```
//!
//! ## Historical Impact
//!
//! **1997: LSTM Invented**
//! - Hochreiter & Schmidhuber
//! - Solved vanishing gradient problem
//! - Enabled long-term memory
//!
//! **2000-2010: Gradual Adoption**
//! - Speech recognition
//! - Handwriting recognition
//! - Limited by compute
//!
//! **2011-2014: Deep Learning Revolution**
//! - GPUs make training feasible
//! - Alex Graves' work on speech/handwriting
//! - Sequence-to-sequence models
//!
//! **2014: GRU Introduced**
//! - Cho et al.
//! - Simpler, faster alternative
//! - Similar performance
//!
//! **2015-2017: Peak LSTM Era**
//! - Google Translate uses LSTM
//! - State-of-the-art in NLP
//! - Most popular sequence model
//!
//! **2017+: Transformer Era**
//! - Attention is All You Need
//! - LSTM usage declining in NLP
//! - Still relevant for streaming/time series
//!
//! ## Code Example Pattern
//!
//! ```rust
//! // Pseudo-code for LSTM in Rust
//!
//! struct LSTMCell {
//!     W_f, W_i, W_o, W_c: Array2<f64>,  // Weight matrices
//!     b_f, b_i, b_o, b_c: Array1<f64>,  // Biases
//! }
//!
//! fn forward(x_t: Array1<f64>, h_prev: Array1<f64>, C_prev: Array1<f64>)
//!     -> (Array1<f64>, Array1<f64>)
//! {
//!     let combined = concatenate(&[h_prev, x_t]);
//!
//!     // Gates
//!     let f_t = sigmoid(W_f.dot(&combined) + b_f);
//!     let i_t = sigmoid(W_i.dot(&combined) + b_i);
//!     let o_t = sigmoid(W_o.dot(&combined) + b_o);
//!     let C_tilde = tanh(W_c.dot(&combined) + b_c);
//!
//!     // Update cell state
//!     let C_t = f_t * C_prev + i_t * C_tilde;
//!
//!     // Output
//!     let h_t = o_t * tanh(C_t);
//!
//!     (h_t, C_t)
//! }
//! ```

fn main() {
    println!("=== LSTM & GRU: Solving the Vanishing Gradient Problem ===\n");

    println!("This example explains LSTM and GRU, the architectures that made");
    println!("long-term sequence learning possible.\n");

    println!("ğŸ“š Key Concepts Covered:");
    println!("  â€¢ Vanishing gradient problem in RNNs");
    println!("  â€¢ LSTM architecture with gates and cell state");
    println!("  â€¢ GRU as a simpler alternative");
    println!("  â€¢ When to use LSTM vs GRU vs Transformers");
    println!("  â€¢ Bidirectional and stacked variants");
    println!("  â€¢ Training tips and best practices\n");

    println!("ğŸ¯ Why This Matters:");
    println!("  â€¢ LSTM enabled the first wave of deep learning for sequences");
    println!("  â€¢ Still relevant for streaming data and resource-constrained settings");
    println!("  â€¢ Foundation for understanding modern sequence models");
    println!("  â€¢ Critical for time series, speech, and online processing\n");

    println!("See the source code documentation for comprehensive explanations!");
}
