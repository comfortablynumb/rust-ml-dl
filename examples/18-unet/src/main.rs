//! # U-Net: Convolutional Networks for Biomedical Image Segmentation
//!
//! This example explains U-Net, the architecture that revolutionized semantic
//! segmentation, especially in medical imaging with limited training data.
//!
//! ## Semantic Segmentation: The Task
//!
//! **Goal:** Classify every pixel in an image
//!
//! ```
//! Image Classification (CNN):
//! Input: 256√ó256√ó3 image
//! Output: Single label ("cat")
//!
//! Object Detection (R-CNN, YOLO):
//! Input: 256√ó256√ó3 image
//! Output: Bounding boxes + labels
//!
//! Semantic Segmentation (U-Net):
//! Input: 256√ó256√ó3 image
//! Output: 256√ó256 pixel-wise labels
//!         ‚Üë Every pixel classified!
//!
//! Example output:
//! [0,0,0,1,1,1,0,0]  ‚Üê 0=background, 1=tumor
//! [0,0,1,1,1,1,1,0]
//! [0,1,1,1,1,1,1,0]
//! [0,0,1,1,1,1,0,0]
//! ```
//!
//! **Applications:**
//! - Medical: Tumor detection, organ segmentation, cell counting
//! - Autonomous driving: Lane detection, pedestrian segmentation
//! - Satellite: Land use classification, building detection
//! - Photography: Background removal, portrait mode
//!
//! ## The Challenge
//!
//! **Problem 1: Resolution**
//! ```
//! Classification CNN:
//! 224√ó224 ‚Üí 112√ó112 ‚Üí 56√ó56 ‚Üí 28√ó28 ‚Üí 14√ó14 ‚Üí 7√ó7 ‚Üí 1√ó1 (class)
//!           ‚Üì
//!        Lose spatial detail (OK for classification!)
//!
//! Segmentation needs:
//! 224√ó224 ‚Üí ... ‚Üí 224√ó224 (pixel-wise predictions)
//!           ‚Üì
//!        Must preserve AND recover spatial detail!
//! ```
//!
//! **Problem 2: Limited Data**
//! ```
//! ImageNet: 1.2M images
//! Medical dataset: Often < 100 images!
//!
//! Need architecture that works with small datasets
//! ```
//!
//! ## U-Net Architecture: The Solution
//!
//! **Key Innovation:** Symmetric encoder-decoder with skip connections
//!
//! ### The "U" Shape
//!
//! ```
//!                    Input (572√ó572√ó1)
//!                           ‚Üì
//!         Contracting Path (Encoder)
//!                           ‚Üì
//!     568√ó568√ó64  ‚Üí  280√ó280√ó128  ‚Üí  136√ó136√ó256
//!         ‚Üì              ‚Üì               ‚Üì
//!     284√ó284√ó64  ‚Üí  140√ó140√ó128  ‚Üí   68√ó68√ó256
//!         ‚Üì              ‚Üì               ‚Üì Bottleneck
//!                    28√ó28√ó512
//!                        ‚Üì
//!         Expanding Path (Decoder)
//!                        ‚Üì
//!     52√ó52√ó256   ‚Üê  100√ó100√ó128  ‚Üê  196√ó196√ó64
//!         ‚Üë              ‚Üë               ‚Üë
//!     104√ó104√ó256 ‚Üê  200√ó200√ó128  ‚Üê  392√ó392√ó64
//!         ‚Üë              ‚Üë               ‚Üë
//!                 Skip Connections
//!                        ‚Üì
//!                Output (388√ó388√ó2)
//! ```
//!
//! ### Three Core Components
//!
//! **1. Contracting Path (Encoder):**
//! ```
//! Purpose: Capture context, extract features
//!
//! Each step:
//! ‚Ä¢ Two 3√ó3 convolutions (ReLU)
//! ‚Ä¢ 2√ó2 max pooling (stride 2, downsample)
//! ‚Ä¢ Double feature channels
//!
//! Example:
//! 572√ó572√ó1 ‚Üí [conv, conv] ‚Üí 568√ó568√ó64
//!           ‚Üí [max pool]   ‚Üí 284√ó284√ó64
//!           ‚Üí [conv, conv] ‚Üí 280√ó280√ó128
//!           ‚Üí [max pool]   ‚Üí 140√ó140√ó128
//!           ...
//!
//! Captures: Edges ‚Üí Textures ‚Üí Parts ‚Üí Objects
//! ```
//!
//! **2. Bottleneck:**
//! ```
//! Smallest spatial size, highest channels
//! Example: 28√ó28√ó512
//!
//! Contains: Most abstract/semantic features
//! ```
//!
//! **3. Expanding Path (Decoder):**
//! ```
//! Purpose: Localization, recover spatial detail
//!
//! Each step:
//! ‚Ä¢ 2√ó2 upconvolution (stride 2, upsample)
//! ‚Ä¢ Concatenate with cropped encoder feature map ‚Üê KEY!
//! ‚Ä¢ Two 3√ó3 convolutions (ReLU)
//! ‚Ä¢ Halve feature channels
//!
//! Example:
//! 28√ó28√ó512 ‚Üí [upconv]     ‚Üí 56√ó56√ó256
//!           ‚Üí [concat]     ‚Üí 56√ó56√ó512 (256+256 from encoder)
//!           ‚Üí [conv, conv] ‚Üí 52√ó52√ó256
//!           ‚Üí [upconv]     ‚Üí 104√ó104√ó128
//!           ...
//! ```
//!
//! ## Skip Connections: The Secret Sauce
//!
//! **Why Skip Connections?**
//! ```
//! Without skips:
//! Encoder ‚Üí Bottleneck ‚Üí Decoder
//!    ‚Üì
//! High-resolution details LOST in bottleneck
//! Decoder struggles to recover precise boundaries
//!
//! With skips:
//! Encoder ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚Üí Decoder (concatenate)
//!    ‚Üì                    ‚Üë
//! Bottleneck ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
//!    ‚Üì
//! Decoder gets both:
//! ‚Ä¢ Semantic info from bottleneck (what)
//! ‚Ä¢ Spatial details from encoder (where)
//! ```
//!
//! **Concrete Example:**
//! ```
//! Task: Segment tumor boundary
//!
//! Encoder features at 100√ó100:
//! ‚Ä¢ Exact pixel locations of edges
//! ‚Ä¢ Fine-grained texture
//! ‚Ä¢ Precise boundaries
//!
//! Bottleneck features at 28√ó28:
//! ‚Ä¢ "This is a tumor" (semantic)
//! ‚Ä¢ Approximate location
//! ‚Ä¢ Missing fine details
//!
//! Decoder at 100√ó100 receives:
//! ‚Ä¢ Bottleneck: "tumor here" (upsampled)
//! ‚Ä¢ Encoder skip: "exact boundary is HERE"
//! ‚Üí Accurate segmentation!
//! ```
//!
//! ## Mathematical Details
//!
//! ### Convolution Layers
//! ```
//! Standard pattern:
//! Conv(3√ó3, ReLU, no padding) ‚Üí Conv(3√ó3, ReLU, no padding)
//!
//! Example:
//! Input: 572√ó572√ó1
//! After conv1: 570√ó570√ó64 (lost 2 pixels per side)
//! After conv2: 568√ó568√ó64 (lost 2 more pixels)
//!
//! Note: Original U-Net uses no padding
//! Modern versions: Use padding to preserve size
//! ```
//!
//! ### Downsampling (Encoder)
//! ```
//! MaxPool(2√ó2, stride=2):
//! 568√ó568√ó64 ‚Üí 284√ó284√ó64
//!
//! Effect:
//! ‚Ä¢ Halve spatial dimensions
//! ‚Ä¢ Keep all channels
//! ‚Ä¢ Reduce parameters
//! ‚Ä¢ Increase receptive field
//! ```
//!
//! ### Upsampling (Decoder)
//! ```
//! Method 1 - Transpose Convolution (Upconv):
//! 28√ó28√ó512 ‚Üí 56√ó56√ó256
//!
//! Learnable upsampling, but can cause checkerboard artifacts
//!
//! Method 2 - Bilinear Interpolation + Conv:
//! 28√ó28√ó512 ‚Üí [interpolate] ‚Üí 56√ó56√ó512
//!           ‚Üí [conv 1√ó1]    ‚Üí 56√ó56√ó256
//!
//! Smoother, no artifacts, commonly used now
//! ```
//!
//! ### Skip Connection Concatenation
//! ```
//! Encoder feature: 136√ó136√ó256
//! Decoder feature: 100√ó100√ó128 (after upconv)
//!
//! Problem: Size mismatch!
//!
//! Solution: Crop encoder feature
//! 136√ó136√ó256 ‚Üí [crop center] ‚Üí 100√ó100√ó256
//!
//! Then concatenate:
//! [100√ó100√ó128, 100√ó100√ó256] ‚Üí 100√ó100√ó384
//!
//! Modern alternative: Use padding to match sizes
//! ```
//!
//! ### Output Layer
//! ```
//! Final conv: 1√ó1 convolution
//! 388√ó388√ó64 ‚Üí 388√ó388√ónum_classes
//!
//! For binary segmentation:
//! 388√ó388√ó64 ‚Üí 388√ó388√ó1 ‚Üí sigmoid ‚Üí probabilities
//!
//! For multi-class:
//! 388√ó388√ó64 ‚Üí 388√ó388√óC ‚Üí softmax ‚Üí class probabilities
//! ```
//!
//! ## Training U-Net
//!
//! ### Loss Functions
//!
//! **1. Pixel-wise Cross-Entropy:**
//! ```
//! Standard choice for segmentation
//!
//! L = -(1/N) Œ£ [y_i log(≈∑_i) + (1-y_i) log(1-≈∑_i)]
//!
//! Where:
//! ‚Ä¢ N = number of pixels
//! ‚Ä¢ y_i = ground truth for pixel i
//! ‚Ä¢ ≈∑_i = predicted probability for pixel i
//!
//! Problem: Imbalanced classes (99% background, 1% tumor)
//! ```
//!
//! **2. Weighted Cross-Entropy:**
//! ```
//! Add weight map to handle:
//! ‚Ä¢ Class imbalance
//! ‚Ä¢ Separation of touching objects
//!
//! L = -(1/N) Œ£ w_i [y_i log(≈∑_i) + (1-y_i) log(1-≈∑_i)]
//!
//! Weight map w_i higher:
//! ‚Ä¢ On rare class pixels
//! ‚Ä¢ At boundaries between objects
//! ```
//!
//! **3. Dice Loss:**
//! ```
//! Based on Dice coefficient (overlap measure)
//!
//! Dice = 2√ó|X ‚à© Y| / (|X| + |Y|)
//!
//! Dice Loss = 1 - Dice
//!
//! Benefits:
//! ‚Ä¢ Handles class imbalance naturally
//! ‚Ä¢ Focus on overlap, not individual pixels
//! ‚Ä¢ Popular in medical imaging
//!
//! Often combined: BCE + Dice
//! ```
//!
//! ### Data Augmentation
//!
//! **Critical for small datasets!**
//!
//! ```
//! Original U-Net paper used:
//! ‚Ä¢ Elastic deformations (simulate tissue variation)
//! ‚Ä¢ Random rotations (0-180¬∞)
//! ‚Ä¢ Random shifts
//! ‚Ä¢ Random scaling
//!
//! Modern additions:
//! ‚Ä¢ Color jittering (brightness, contrast)
//! ‚Ä¢ Gaussian noise
//! ‚Ä¢ Blur
//! ‚Ä¢ Random crops
//! ‚Ä¢ Horizontal/vertical flips
//!
//! Can train on < 30 images with heavy augmentation!
//! ```
//!
//! ### Optimization
//!
//! ```
//! Optimizer: Adam or SGD with momentum
//! Learning rate: 0.0001 - 0.001
//! Batch size: 1-8 (often limited by GPU memory)
//! Epochs: 100-500 (early stopping on validation)
//!
//! Learning rate schedule:
//! ‚Ä¢ ReduceLROnPlateau: Halve LR when validation plateaus
//! ‚Ä¢ Cosine annealing
//! ‚Ä¢ Step decay
//! ```
//!
//! ## U-Net Variants
//!
//! ### U-Net++ (Nested U-Net)
//! ```
//! Enhancement: Dense skip connections at multiple scales
//!
//!     Encoder
//!        ‚Üì‚Üò
//!        ‚Üì ‚Üò‚Üí Dense connections
//!        ‚Üì   ‚Üò
//!    Bottleneck
//!        ‚Üë   ‚Üó
//!        ‚Üë ‚Üó‚Üí Multiple paths
//!        ‚Üë‚Üó
//!     Decoder
//!
//! Benefits:
//! ‚Ä¢ Better gradient flow
//! ‚Ä¢ Multi-scale feature fusion
//! ‚Ä¢ Slight accuracy improvement
//! ```
//!
//! ### Attention U-Net
//! ```
//! Add attention gates to skip connections:
//!
//! Encoder feature ‚Üí Attention Gate ‚Üí Weighted feature
//!                        ‚Üë
//!                   Decoder feature
//!
//! Attention gate:
//! ‚Ä¢ Highlights relevant regions
//! ‚Ä¢ Suppresses irrelevant features
//! ‚Ä¢ Learns where to focus
//!
//! Benefits:
//! ‚Ä¢ Better for complex images
//! ‚Ä¢ Handles variable object sizes
//! ‚Ä¢ Popular in medical imaging
//! ```
//!
//! ### 3D U-Net
//! ```
//! Extend to volumetric data (CT, MRI scans):
//!
//! 2D: Conv(3√ó3) ‚Üí MaxPool(2√ó2) ‚Üí Upconv(2√ó2)
//! 3D: Conv(3√ó3√ó3) ‚Üí MaxPool(2√ó2√ó2) ‚Üí Upconv(2√ó2√ó2)
//!
//! Input: 128√ó128√ó128√ó1 (3D volume)
//! Output: 128√ó128√ó128√óC (volumetric segmentation)
//!
//! Applications:
//! ‚Ä¢ Organ segmentation in CT
//! ‚Ä¢ Brain tumor in MRI
//! ‚Ä¢ Video object segmentation
//! ```
//!
//! ### Residual U-Net
//! ```
//! Replace plain convolutions with ResNet blocks:
//!
//! Instead of: Conv ‚Üí Conv
//! Use:        ResBlock (conv + skip connection)
//!
//! Benefits:
//! ‚Ä¢ Easier to train deeper networks
//! ‚Ä¢ Better gradient flow
//! ‚Ä¢ Slight performance gain
//! ```
//!
//! ## Applications & Success Stories
//!
//! ### Medical Imaging
//!
//! **Cell Segmentation (Original paper 2015):**
//! ```
//! Dataset: Neuronal structures in EM images
//! Training: 30 images (512√ó512)
//! Result: Won ISBI 2015 challenge
//! Dice score: 0.92 (vs 0.88 previous best)
//!
//! Key: Heavy data augmentation!
//! ```
//!
//! **Tumor Detection:**
//! ```
//! Brain tumors (MRI):
//! ‚Ä¢ 4 classes: healthy, edema, enhancing, non-enhancing
//! ‚Ä¢ Dice: 0.88-0.91
//! ‚Ä¢ Helps radiologists find small tumors
//!
//! Lung nodules (CT):
//! ‚Ä¢ Cancer screening
//! ‚Ä¢ Dice: 0.85-0.90
//! ‚Ä¢ Reduces false positives
//! ```
//!
//! **Organ Segmentation:**
//! ```
//! Liver, kidney, spleen from CT:
//! ‚Ä¢ Automate surgical planning
//! ‚Ä¢ Dice: 0.94-0.96
//! ‚Ä¢ Save radiologist hours of manual work
//! ```
//!
//! ### Autonomous Driving
//!
//! **Road Scene Segmentation:**
//! ```
//! Cityscapes dataset:
//! ‚Ä¢ 19 classes: road, car, person, etc.
//! ‚Ä¢ Real-time processing needed
//! ‚Ä¢ mIoU: 70-80% (IoU = Intersection over Union)
//!
//! Use: Lane keeping, obstacle avoidance
//! ```
//!
//! ### Satellite Imagery
//!
//! **Land Use Classification:**
//! ```
//! Classes: urban, forest, water, agriculture
//! ‚Ä¢ Large-scale mapping
//! ‚Ä¢ Environmental monitoring
//! ‚Ä¢ Urban planning
//! ```
//!
//! **Building Detection:**
//! ```
//! Segment buildings from aerial photos
//! ‚Ä¢ Disaster response
//! ‚Ä¢ Infrastructure mapping
//! ‚Ä¢ F1 score: 0.85-0.90
//! ```
//!
//! ### Photography & Art
//!
//! **Portrait Segmentation:**
//! ```
//! Separate person from background:
//! ‚Ä¢ Portrait mode blur
//! ‚Ä¢ Background replacement
//! ‚Ä¢ Real-time on mobile (optimized U-Net)
//! ```
//!
//! **Image Inpainting:**
//! ```
//! Remove objects, fill in background
//! ‚Ä¢ Photoshop-style content-aware fill
//! ‚Ä¢ Old photo restoration
//! ```
//!
//! ## Performance Metrics
//!
//! ### IoU (Intersection over Union)
//! ```
//! IoU = Area of Overlap / Area of Union
//!
//! Example:
//! Ground truth: ‚ñ†‚ñ†‚ñ†‚ñ°
//! Prediction:    ‚ñ°‚ñ†‚ñ†‚ñ†
//! Overlap:       ‚ñ°‚ñ†‚ñ†‚ñ° (2 pixels)
//! Union:        ‚ñ†‚ñ†‚ñ†‚ñ† (4 pixels)
//! IoU = 2/4 = 0.5
//!
//! Range: 0 (no overlap) to 1 (perfect)
//! Good: > 0.7, Great: > 0.85
//! ```
//!
//! ### Dice Coefficient
//! ```
//! Dice = 2√ó|Overlap| / (|Prediction| + |Truth|)
//!
//! Example:
//! Ground truth: 10 pixels
//! Prediction: 12 pixels
//! Overlap: 8 pixels
//! Dice = 2√ó8/(10+12) = 16/22 = 0.73
//!
//! Similar to IoU, often used in medical imaging
//! ```
//!
//! ### Pixel Accuracy
//! ```
//! Accuracy = Correct pixels / Total pixels
//!
//! Problem: Misleading with imbalance
//! Example: 95% background, 5% tumor
//! Predict all background: 95% accuracy! ‚ùå
//!
//! Use IoU or Dice instead for segmentation
//! ```
//!
//! ## Modern Developments
//!
//! ### Transformers for Segmentation
//! ```
//! SegFormer, Swin-UNet (2021-2022):
//! ‚Ä¢ Replace CNN encoder with Vision Transformer
//! ‚Ä¢ Better long-range dependencies
//! ‚Ä¢ State-of-the-art results
//!
//! But:
//! ‚Ä¢ Need more data than U-Net
//! ‚Ä¢ Slower inference
//! ‚Ä¢ Higher memory
//!
//! U-Net still preferred for:
//! ‚Ä¢ Medical imaging (small datasets)
//! ‚Ä¢ Real-time applications
//! ‚Ä¢ Resource-constrained settings
//! ```
//!
//! ### Efficient U-Net
//! ```
//! MobileNet-UNet:
//! ‚Ä¢ Replace encoder with MobileNetV2
//! ‚Ä¢ Depthwise separable convolutions
//! ‚Ä¢ 10√ó fewer parameters
//! ‚Ä¢ 3√ó faster inference
//! ‚Ä¢ Minimal accuracy loss
//!
//! Use: Mobile apps, edge devices
//! ```
//!
//! ### Foundation Models
//! ```
//! Segment Anything (SAM, Meta 2023):
//! ‚Ä¢ U-Net-like architecture
//! ‚Ä¢ Trained on 1B masks
//! ‚Ä¢ Zero-shot segmentation
//! ‚Ä¢ Click ‚Üí instant segmentation
//!
//! Revolution: No training needed!
//! ```
//!
//! ## Implementation Tips
//!
//! ### Memory Management
//! ```
//! U-Net is memory-hungry:
//! ‚Ä¢ Stores features at every level for skips
//! ‚Ä¢ Large input images ‚Üí huge memory
//!
//! Solutions:
//! 1. Reduce batch size (even 1 works!)
//! 2. Use smaller input patches
//! 3. Mixed precision training (FP16)
//! 4. Gradient checkpointing
//! 5. Use smaller base channels (32 instead of 64)
//! ```
//!
//! ### Input Size Considerations
//! ```
//! Original: 572√ó572 ‚Üí 388√ó388 (size reduction)
//! Modern: Use padding ‚Üí same size input/output
//!
//! Patch-based for large images:
//! 2048√ó2048 image ‚Üí 256√ó256 patches
//! Segment each patch ‚Üí stitch together
//! Add overlap to avoid boundary artifacts
//! ```
//!
//! ### Batch Normalization
//! ```
//! Add after each convolution:
//! Conv ‚Üí BatchNorm ‚Üí ReLU
//!
//! Benefits:
//! ‚Ä¢ Faster training
//! ‚Ä¢ Higher learning rates possible
//! ‚Ä¢ Better generalization
//! ‚Ä¢ Less sensitive to initialization
//!
//! Modern standard in U-Net variants
//! ```
//!
//! ## Historical Impact
//!
//! **2015:** U-Net paper published
//! - Won ISBI cell tracking challenge
//! - 30 training images ‚Üí state-of-the-art
//! - Showed power of augmentation + architecture
//!
//! **2016-2018:** Rapid adoption
//! - Became standard for medical segmentation
//! - Kaggle competitions
//! - 10,000+ citations
//!
//! **2019-2020:** Variants flourish
//! - U-Net++, Attention U-Net, 3D U-Net
//! - Applied beyond medical imaging
//! - Autonomous driving, satellite imagery
//!
//! **2021+:** Still relevant
//! - 40,000+ citations (most cited segmentation paper)
//! - Benchmark for new methods
//! - Foundation for modern architectures
//! - SAM (2023) uses U-Net-like design
//!
//! **Legacy:**
//! - Proof that architecture matters
//! - Skip connections now standard everywhere
//! - Encoder-decoder paradigm ubiquitous

fn main() {
    println!("=== U-Net: Semantic Segmentation Architecture ===\n");

    println!("This example explains U-Net, the architecture that revolutionized");
    println!("image segmentation, especially with limited training data.\n");

    println!("üìö Key Concepts Covered:");
    println!("  ‚Ä¢ Semantic segmentation vs classification");
    println!("  ‚Ä¢ Encoder-decoder architecture");
    println!("  ‚Ä¢ Skip connections for spatial detail");
    println!("  ‚Ä¢ Training with small medical datasets");
    println!("  ‚Ä¢ Data augmentation strategies");
    println!("  ‚Ä¢ Dice loss and IoU metrics\n");

    println!("üéØ Why This Matters:");
    println!("  ‚Ä¢ Revolutionized medical image analysis");
    println!("  ‚Ä¢ Enabled segmentation with < 30 training images");
    println!("  ‚Ä¢ Standard architecture for pixel-wise prediction");
    println!("  ‚Ä¢ Applied to autonomous driving, satellite imagery, photography");
    println!("  ‚Ä¢ Foundation for modern segmentation models\n");

    println!("See the source code documentation for comprehensive explanations!");
}
