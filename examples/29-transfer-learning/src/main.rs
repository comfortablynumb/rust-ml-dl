//! # Transfer Learning & Fine-Tuning: The Most Practical Deep Learning Workflow
//!
//! Transfer learning is how practitioners actually use deep learning in production.
//! Train on large dataset (ImageNet, BERT), then adapt to your specific task.
//!
//! ## The Core Idea
//!
//! ```
//! Traditional: Train from scratch on your data
//! ‚Ä¢ Need millions of samples
//! ‚Ä¢ Requires weeks of training
//! ‚Ä¢ Often impossible for small datasets
//!
//! Transfer Learning: Start from pre-trained model
//! ‚Ä¢ Uses knowledge from large dataset (ImageNet, Wikipedia)
//! ‚Ä¢ Fine-tune on your data (hundreds to thousands of samples)
//! ‚Ä¢ Train in hours instead of weeks
//! ‚Ä¢ Often better performance!
//! ```
//!
//! ## Why Transfer Learning Works
//!
//! ### Feature Hierarchy
//!
//! ```
//! Deep networks learn hierarchical features:
//!
//! Layer 1 (Early): Edges, colors, textures
//! ‚Ä¢ Universal! Same for all vision tasks
//! ‚Ä¢ Horizontal/vertical edges
//! ‚Ä¢ Color blobs
//!
//! Layer 2-3 (Middle): Shapes, patterns
//! ‚Ä¢ Circles, rectangles
//! ‚Ä¢ Simple textures
//! ‚Ä¢ Mostly transferable
//!
//! Layer 4-5 (Late): Task-specific features
//! ‚Ä¢ Cat ears, dog noses (for ImageNet)
//! ‚Ä¢ Your task: Different high-level features
//! ‚Ä¢ Need to adapt these layers
//! ```
//!
//! ### Intuition
//!
//! ```
//! Learning to classify images is like learning to paint:
//!
//! Traditional: Learn to paint from scratch
//! ‚Ä¢ Learn to hold brush
//! ‚Ä¢ Learn color mixing
//! ‚Ä¢ Learn composition
//! ‚Ä¢ Takes years!
//!
//! Transfer Learning: Start with painting skills
//! ‚Ä¢ Already know techniques
//! ‚Ä¢ Just learn your specific style
//! ‚Ä¢ Takes weeks, not years
//! ```
//!
//! ## Two Main Approaches
//!
//! ### 1. Feature Extraction (Freeze Early Layers)
//!
//! ```
//! Use pre-trained model as fixed feature extractor:
//!
//! Pre-trained Model:
//! Input ‚Üí Conv1 ‚Üí Conv2 ‚Üí ... ‚Üí ConvN ‚Üí FC
//!         ‚Üì      ‚Üì           ‚Üì       ‚Üì
//!       Freeze Freeze     Freeze   Train (new)
//!
//! Only train:
//! ‚Ä¢ Final classification layer
//! ‚Ä¢ Maybe last conv block
//!
//! When to use:
//! ‚úÖ Small dataset (< 1000 samples)
//! ‚úÖ Similar to pre-training task
//! ‚úÖ Limited compute
//! ‚úÖ Fast training needed
//! ```
//!
//! **Example:**
//! ```
//! Pre-trained: ResNet-50 on ImageNet (1.2M images, 1000 classes)
//! Your task: Classify 5 types of flowers (500 images)
//!
//! Approach:
//! 1. Load ResNet-50 weights
//! 2. Remove final layer (1000 classes)
//! 3. Add new layer (5 classes)
//! 4. Freeze all layers except last
//! 5. Train only the new layer (2048 ‚Üí 5)
//!
//! Training time: Minutes instead of hours!
//! ```
//!
//! ### 2. Fine-Tuning (Train All or Later Layers)
//!
//! ```
//! Gradually unfreeze and train more layers:
//!
//! Stage 1: Train only new head
//! Input ‚Üí [Frozen CNN] ‚Üí [New FC] ‚Üê Train
//!
//! Stage 2: Unfreeze later conv blocks
//! Input ‚Üí [Frozen] ‚Üí [Train] ‚Üí [Train]
//!
//! Stage 3 (optional): Train all layers
//! Input ‚Üí [Train] ‚Üí [Train] ‚Üí [Train]
//!
//! When to use:
//! ‚úÖ Medium to large dataset (> 10K samples)
//! ‚úÖ Different from pre-training task
//! ‚úÖ Want best possible performance
//! ‚úÖ Have compute budget
//! ```
//!
//! **Learning Rates:**
//! ```
//! Different layers = different learning rates!
//!
//! Early layers (frozen or tiny LR):
//! ‚Ä¢ LR = 0 (frozen) or 1e-5 (very small)
//! ‚Ä¢ Already learned universal features
//!
//! Middle layers (small LR):
//! ‚Ä¢ LR = 1e-4 to 1e-3
//! ‚Ä¢ Adapt features to your domain
//!
//! New layers (normal LR):
//! ‚Ä¢ LR = 1e-3 to 1e-2
//! ‚Ä¢ Random initialization, need more updates
//!
//! This is called "discriminative learning rates"
//! ```
//!
//! ## Computer Vision Transfer Learning
//!
//! ### Popular Pre-trained Models
//!
//! ```
//! ImageNet Pre-trained Models:
//!
//! ResNet Family:
//! ‚Ä¢ ResNet-18: 11M params, fast
//! ‚Ä¢ ResNet-50: 25M params, good balance
//! ‚Ä¢ ResNet-101: 44M params, best accuracy
//!
//! EfficientNet:
//! ‚Ä¢ EfficientNet-B0: 5M params, efficient
//! ‚Ä¢ EfficientNet-B7: 66M params, SOTA
//!
//! Vision Transformers:
//! ‚Ä¢ ViT-Base: 86M params
//! ‚Ä¢ ViT-Large: 307M params
//!
//! Choice depends on:
//! ‚Ä¢ Dataset size: Larger data ‚Üí larger model
//! ‚Ä¢ Compute: Mobile ‚Üí small, server ‚Üí large
//! ‚Ä¢ Speed requirements: Real-time ‚Üí small
//! ```
//!
//! ### Typical Workflow
//!
//! ```
//! 1. Choose pre-trained model
//!    dataset < 5K: ResNet-18 or EfficientNet-B0
//!    dataset > 50K: ResNet-50 or EfficientNet-B4
//!
//! 2. Replace final layer
//!    model.fc = Linear(2048, num_classes)  # For ResNet-50
//!
//! 3. Train in stages:
//!    
//!    Stage 1 (2-5 epochs): Freeze all, train head
//!    for param in model.parameters():
//!        param.requires_grad = False
//!    model.fc.requires_grad = True
//!    
//!    Stage 2 (10-20 epochs): Unfreeze all, small LR
//!    for param in model.parameters():
//!        param.requires_grad = True
//!    optimizer = Adam(model.parameters(), lr=1e-4)
//!
//! 4. Data augmentation (critical!)
//!    transforms = [
//!        RandomCrop(224),
//!        RandomHorizontalFlip(),
//!        ColorJitter(),
//!        Normalize(ImageNet_mean, ImageNet_std)  ‚Üê Use same normalization!
//!    ]
//! ```
//!
//! ## NLP Transfer Learning
//!
//! ### Pre-trained Language Models
//!
//! ```
//! BERT (Bidirectional Encoder):
//! ‚Ä¢ Pre-trained on Wikipedia + Books
//! ‚Ä¢ 110M (base) to 340M (large) params
//! ‚Ä¢ Best for: Classification, QA, NER
//!
//! GPT-2/3 (Autoregressive Decoder):
//! ‚Ä¢ Pre-trained on web text
//! ‚Ä¢ 117M to 175B params
//! ‚Ä¢ Best for: Generation, completion
//!
//! T5 (Encoder-Decoder):
//! ‚Ä¢ Pre-trained on C4 dataset
//! ‚Ä¢ 60M to 11B params
//! ‚Ä¢ Best for: Translation, summarization
//!
//! RoBERTa (Improved BERT):
//! ‚Ä¢ Better training procedure
//! ‚Ä¢ Often outperforms BERT
//! ```
//!
//! ### Fine-Tuning BERT Example
//!
//! ```
//! Task: Sentiment classification
//!
//! 1. Load pre-trained BERT
//!    model = BertForSequenceClassification(num_labels=2)
//!
//! 2. Add task-specific head (already included)
//!    [CLS] token ‚Üí Linear(768, 2) ‚Üí Softmax
//!
//! 3. Fine-tune entire model
//!    All layers trainable!
//!    LR = 2e-5 to 5e-5 (very small!)
//!
//! 4. Train for few epochs (2-4)
//!    BERT already knows language
//!    Just adapting to your task
//!
//! 5. Careful with overfitting
//!    Early stopping essential
//!    Dropout = 0.1
//! ```
//!
//! ## Domain Adaptation
//!
//! **When source and target domains differ**
//!
//! ### Strategy 1: Gradual Unfreezing
//!
//! ```
//! Epoch 1-2: Train head only
//! Epoch 3-5: Unfreeze last block
//! Epoch 6-10: Unfreeze second-to-last block
//! ...
//!
//! This is "progressive unfreezing"
//! Used in ULMFit, works well for NLP
//! ```
//!
//! ### Strategy 2: Discriminative Fine-Tuning
//!
//! ```
//! Different learning rates per layer:
//!
//! optimizer = [
//!     {'params': model.layer1, 'lr': 1e-5},
//!     {'params': model.layer2, 'lr': 1e-4},
//!     {'params': model.layer3, 'lr': 1e-3},
//!     {'params': model.head, 'lr': 1e-2}
//! ]
//!
//! Early layers: Learn slowly (preserve knowledge)
//! Later layers: Learn faster (adapt to task)
//! ```
//!
//! ### Strategy 3: Two-Stage Training
//!
//! ```
//! Stage 1: Feature extraction (frozen)
//! ‚Ä¢ Fast, prevents catastrophic forgetting
//! ‚Ä¢ Gets head to reasonable state
//!
//! Stage 2: Full fine-tuning
//! ‚Ä¢ Slower, with small LR
//! ‚Ä¢ Adapts entire network
//! ```
//!
//! ## Best Practices
//!
//! ### Data Preprocessing
//!
//! ```
//! ‚ö†Ô∏è Critical: Use same normalization as pre-training!
//!
//! ImageNet normalization:
//! mean = [0.485, 0.456, 0.406]
//! std = [0.229, 0.224, 0.225]
//!
//! Bad: Different normalization
//! ‚Üí Pre-trained features don't work!
//!
//! Good: Match pre-training exactly
//! ‚Üí Transferable features
//! ```
//!
//! ### Learning Rate Selection
//!
//! ```
//! Rule of thumb:
//!
//! From scratch: LR = 1e-3 to 1e-2
//! Fine-tuning: LR = 1e-5 to 1e-4
//!               ‚Üë 10-100√ó smaller!
//!
//! Why smaller?
//! ‚Ä¢ Pre-trained weights already good
//! ‚Ä¢ Don't want to destroy learned features
//! ‚Ä¢ "Fine" tuning, not "coarse" tuning
//!
//! LR schedule:
//! ‚Ä¢ Warmup: Gradually increase LR
//! ‚Ä¢ Decay: Reduce LR when plateau
//! ‚Ä¢ Cosine annealing: Smooth decay
//! ```
//!
//! ### When Transfer Learning Fails
//!
//! ```
//! ‚ùå Very different domains:
//!    ImageNet (natural images) ‚Üí X-rays
//!    Solution: Find domain-specific pre-trained model
//!
//! ‚ùå Very different tasks:
//!    Classification ‚Üí Segmentation
//!    Solution: Use encoder only, retrain decoder
//!
//! ‚ùå Tiny dataset (< 100 samples):
//!    Solution: Freeze more layers, heavy augmentation
//!
//! ‚ùå Wrong normalization:
//!    Solution: Match pre-training exactly
//! ```
//!
//! ## Advanced Techniques
//!
//! ### Multi-Task Learning
//!
//! ```
//! Share encoder across multiple tasks:
//!
//! Shared Encoder
//!    ‚Üì      ‚Üì      ‚Üì
//! Task1  Task2  Task3
//! (head) (head) (head)
//!
//! Benefits:
//! ‚Ä¢ Better representations
//! ‚Ä¢ Data efficiency
//! ‚Ä¢ Regularization effect
//! ```
//!
//! ### Knowledge Distillation + Transfer
//!
//! ```
//! Large teacher model (pre-trained)
//!    ‚Üì
//! Distill to small student
//!    ‚Üì
//! Fine-tune student on target task
//!
//! Get: Small model + good performance
//! ```
//!
//! ### Self-Supervised Pre-training
//!
//! ```
//! Instead of ImageNet:
//! ‚Ä¢ SimCLR: Contrastive learning
//! ‚Ä¢ MAE: Masked autoencoders
//! ‚Ä¢ MoCo: Momentum contrast
//!
//! On your domain data (unlabeled)!
//! Then fine-tune on labeled subset
//! ```
//!
//! ## Real-World Examples
//!
//! ### Medical Imaging
//!
//! ```
//! Pre-training: ImageNet (natural images)
//! Target: X-ray classification
//!
//! Approach:
//! 1. ImageNet pre-trained ResNet-50
//! 2. Fine-tune on ChestX-ray dataset
//! 3. Heavy augmentation (rotation, zoom)
//! 4. Small LR (1e-4), long training
//!
//! Result: 85% ‚Üí 92% accuracy vs from scratch
//! ```
//!
//! ### Sentiment Analysis
//!
//! ```
//! Pre-training: BERT on Wikipedia
//! Target: Movie review sentiment
//!
//! Approach:
//! 1. Load BERT-base
//! 2. Add classification head
//! 3. Fine-tune 3 epochs, LR=2e-5
//! 4. Early stopping
//!
//! Result: 89% accuracy with 5K samples
//! From scratch: Would need 50K+ samples
//! ```
//!
//! ### Object Detection
//!
//! ```
//! Pre-training: ImageNet classification
//! Target: Custom object detection
//!
//! Approach:
//! 1. YOLO with ResNet-50 backbone
//! 2. Keep backbone frozen initially
//! 3. Train detection head (10 epochs)
//! 4. Unfreeze all, fine-tune (20 epochs)
//!
//! Result: Detect custom objects with 1K images
//! ```
//!
//! ## Measuring Success
//!
//! ```
//! Compare:
//! ‚Ä¢ From scratch baseline
//! ‚Ä¢ Transfer learning (feature extraction)
//! ‚Ä¢ Transfer learning (fine-tuning)
//!
//! Metrics:
//! ‚Ä¢ Accuracy on test set
//! ‚Ä¢ Training time
//! ‚Ä¢ Convergence speed
//! ‚Ä¢ Data efficiency
//!
//! Transfer learning should:
//! ‚úÖ Converge faster (fewer epochs)
//! ‚úÖ Reach higher accuracy
//! ‚úÖ Need less data
//! ‚úÖ Be more stable
//! ```

fn main() {
    println!("=== Transfer Learning & Fine-Tuning ===\n");

    println!("The most practical deep learning workflow: Start from pre-trained models.\n");

    println!("üìö Key Concepts:");
    println!("  ‚Ä¢ Feature Extraction: Freeze early layers, train head");
    println!("  ‚Ä¢ Fine-Tuning: Train all layers with small LR");
    println!("  ‚Ä¢ Discriminative LR: Different rates per layer");
    println!("  ‚Ä¢ Domain Adaptation: Adapt to different data\n");

    println!("üéØ Why It Works:");
    println!("  ‚Ä¢ Early layers learn universal features (edges, textures)");
    println!("  ‚Ä¢ Later layers learn task-specific features");
    println!("  ‚Ä¢ Transfer knowledge from large datasets (ImageNet, Wikipedia)");
    println!("  ‚Ä¢ Train with 100√ó less data\n");

    println!("üí° Typical Workflow:");
    println!("  1. Load pre-trained model (ResNet-50, BERT)");
    println!("  2. Replace final layer for your task");
    println!("  3. Stage 1: Train head only (2-5 epochs)");
    println!("  4. Stage 2: Fine-tune all layers, small LR (10-20 epochs)");
    println!("  5. Use heavy data augmentation\n");

    println!("üîß Popular Models:");
    println!("  ‚Ä¢ Vision: ResNet, EfficientNet, ViT");
    println!("  ‚Ä¢ NLP: BERT, RoBERTa, GPT-2, T5");
    println!("  ‚Ä¢ Multi-modal: CLIP\n");

    println!("See source code documentation for comprehensive explanations!");
}
