//! # Regularization & Dropout: Preventing Overfitting
//!
//! Comprehensive guide to regularization techniques that prevent overfitting
//! and improve model generalization.
//!
//! ## The Overfitting Problem
//!
//! ```
//! Training: 99% accuracy ‚úì
//! Test: 60% accuracy ‚ùå
//!
//! Model memorized training data but doesn't generalize!
//!
//! Signs of overfitting:
//! ‚Ä¢ Training loss decreases, validation loss increases
//! ‚Ä¢ Large gap between train and test performance
//! ‚Ä¢ Model too complex for dataset size
//! ‚Ä¢ High variance (sensitive to training data)
//! ```
//!
//! ## Weight Regularization (L1 & L2)
//!
//! **Add penalty to loss function for large weights**
//!
//! ### L2 Regularization (Ridge, Weight Decay)
//!
//! ```
//! Modified loss:
//! L_total = L_data + Œª √ó Œ£ w¬≤
//!
//! Where:
//! ‚Ä¢ L_data: Original loss (cross-entropy, MSE, etc.)
//! ‚Ä¢ Œª: Regularization strength (0.0001 - 0.01)
//! ‚Ä¢ Œ£ w¬≤: Sum of squared weights
//!
//! Effect on gradients:
//! ‚àÇL_total/‚àÇw = ‚àÇL_data/‚àÇw + 2Œªw
//!
//! Update rule:
//! w = w - lr √ó (‚àÇL_data/‚àÇw + 2Œªw)
//!   = (1 - 2Œª√ólr) √ó w - lr √ó ‚àÇL_data/‚àÇw
//!       ‚Üë
//!   Weight decay!
//! ```
//!
//! **Why it works:**
//! ```
//! ‚Ä¢ Prevents weights from growing too large
//! ‚Ä¢ Prefers simpler models (Occam's razor)
//! ‚Ä¢ Smooths decision boundaries
//! ‚Ä¢ Distributes weights more evenly
//!
//! Example:
//! Without L2: [0, 0, 100, 0, 0]  ‚Üê Relies on single feature
//! With L2: [10, 15, 20, 12, 8]  ‚Üê Uses multiple features
//! ```
//!
//! **Typical values:**
//! ```
//! Œª = 0.0001: Light regularization
//! Œª = 0.001: Medium (common default)
//! Œª = 0.01: Strong regularization
//! ```
//!
//! ### L1 Regularization (Lasso)
//!
//! ```
//! Modified loss:
//! L_total = L_data + Œª √ó Œ£ |w|
//!
//! Gradient:
//! ‚àÇL_total/‚àÇw = ‚àÇL_data/‚àÇw + Œª √ó sign(w)
//!
//! Effect:
//! ‚Ä¢ Pushes weights exactly to zero
//! ‚Ä¢ Creates sparse models
//! ‚Ä¢ Feature selection (some weights become 0)
//! ```
//!
//! **L1 vs L2:**
//! ```
//! L1:
//! ‚úÖ Sparse solutions (feature selection)
//! ‚úÖ Interpretable (fewer non-zero weights)
//! ‚ùå Not differentiable at 0
//!
//! L2:
//! ‚úÖ Smooth optimization
//! ‚úÖ Better gradient properties
//! ‚úÖ More commonly used in deep learning
//! ‚ùå Doesn't create sparsity
//!
//! Elastic Net: L1 + L2 (best of both)
//! L_total = L_data + Œª‚ÇÅ Œ£ |w| + Œª‚ÇÇ Œ£ w¬≤
//! ```
//!
//! ## Dropout (2014)
//!
//! **Randomly drop neurons during training**
//!
//! ### How It Works
//!
//! ```
//! Training:
//! For each training sample:
//!   For each neuron:
//!     With probability p: Set output to 0
//!     With probability 1-p: Keep active
//!
//! Typical p = 0.5 (drop 50% of neurons)
//!
//! Example forward pass:
//! Before dropout: [0.5, 0.8, 0.3, 0.9, 0.2]
//! After dropout:  [0.0, 0.8, 0.0, 0.9, 0.2]  ‚Üê Random!
//!                  ‚Üë        ‚Üë
//!              Dropped   Dropped
//! ```
//!
//! ### Inverted Dropout
//!
//! ```
//! Training:
//! mask = random(0,1) > p
//! output = (input * mask) / (1-p)
//!          ‚Üë               ‚Üë
//!       Random drop    Scale up to maintain expected value
//!
//! Inference:
//! output = input  ‚Üê No dropout, no scaling
//!
//! Example with p=0.5:
//! Training: [1.0, 0, 2.0, 0] / 0.5 = [2.0, 0, 4.0, 0]
//! Inference: [1.0, 1.0, 2.0, 2.0]  ‚Üê All neurons active
//!
//! Expected value matches!
//! ```
//!
//! ### Why Dropout Works
//!
//! ```
//! 1. Ensemble Effect:
//!    Each forward pass = different sub-network
//!    Training 1000 mini-batches = 1000 different networks!
//!    Inference = averaging all networks
//!
//! 2. Co-adaptation Prevention:
//!    Without dropout: Neurons rely on specific other neurons
//!    With dropout: Each neuron must be independently useful
//!    ‚Üí More robust features
//!
//! 3. Noise Injection:
//!    Acts as regularization
//!    Forces network to learn redundant representations
//! ```
//!
//! ### Dropout Rates by Layer
//!
//! ```
//! Input layer: 0.1-0.2 (light dropout)
//! Hidden layers: 0.3-0.5 (moderate to heavy)
//! Output layer: 0.0 (no dropout)
//!
//! Rule of thumb:
//! ‚Ä¢ Larger layers: Higher dropout (0.5)
//! ‚Ä¢ Smaller layers: Lower dropout (0.2)
//! ‚Ä¢ CNNs: Lower dropout (0.1-0.3)
//! ‚Ä¢ Fully connected: Higher dropout (0.5)
//! ```
//!
//! ## DropConnect
//!
//! ```
//! Dropout: Drop neurons (activations)
//! DropConnect: Drop weights
//!
//! Training:
//! For each forward pass:
//!   Randomly drop weight connections
//!   M ~ Bernoulli(1-p)
//!   y = œÉ((W ‚äô M) √ó x)
//!
//! Effect:
//! ‚Ä¢ More fine-grained than dropout
//! ‚Ä¢ Can be more effective
//! ‚Ä¢ Computationally more expensive
//! ‚Ä¢ Less commonly used than dropout
//! ```
//!
//! ## Early Stopping
//!
//! **Stop training when validation performance degrades**
//!
//! ```
//! Algorithm:
//! best_val_loss = ‚àû
//! patience_counter = 0
//! patience = 10  ‚Üê How many epochs to wait
//!
//! For each epoch:
//!   Train on training set
//!   Evaluate on validation set
//!   
//!   if val_loss < best_val_loss:
//!     best_val_loss = val_loss
//!     save_model()  ‚Üê Checkpoint
//!     patience_counter = 0
//!   else:
//!     patience_counter += 1
//!   
//!   if patience_counter >= patience:
//!     break  ‚Üê Stop training
//!
//! Load best model from checkpoint
//! ```
//!
//! **Benefits:**
//! ```
//! ‚úÖ Simple and effective
//! ‚úÖ No hyperparameters to tune (just patience)
//! ‚úÖ Works with any model
//! ‚úÖ Prevents overfitting automatically
//! ```
//!
//! **Best Practices:**
//! ```
//! ‚Ä¢ Patience: 5-20 epochs (longer for large models)
//! ‚Ä¢ Always save best model
//! ‚Ä¢ Monitor validation loss, not accuracy
//! ‚Ä¢ Use separate validation set
//! ‚Ä¢ Combine with other regularization
//! ```
//!
//! ## Data Augmentation
//!
//! **Create more training data through transformations**
//!
//! ### Image Augmentation
//!
//! ```
//! Geometric:
//! ‚Ä¢ Random crop (224√ó224 from 256√ó256)
//! ‚Ä¢ Horizontal flip (p=0.5)
//! ‚Ä¢ Rotation (¬±15¬∞)
//! ‚Ä¢ Scaling (0.8-1.2√ó)
//! ‚Ä¢ Translation (shift)
//! ‚Ä¢ Shear
//!
//! Photometric:
//! ‚Ä¢ Brightness adjustment (¬±20%)
//! ‚Ä¢ Contrast adjustment (¬±20%)
//! ‚Ä¢ Saturation adjustment
//! ‚Ä¢ Hue adjustment
//! ‚Ä¢ Gaussian noise
//! ‚Ä¢ Gaussian blur
//!
//! Advanced:
//! ‚Ä¢ Cutout (random patches set to 0)
//! ‚Ä¢ Mixup (blend two images)
//! ‚Ä¢ CutMix (paste patch from another image)
//! ‚Ä¢ AutoAugment (learned augmentation policy)
//! ‚Ä¢ RandAugment (random augmentation chain)
//! ```
//!
//! ### Text Augmentation
//!
//! ```
//! ‚Ä¢ Synonym replacement (word ‚Üí similar word)
//! ‚Ä¢ Random insertion (add words)
//! ‚Ä¢ Random swap (swap word positions)
//! ‚Ä¢ Random deletion (remove words)
//! ‚Ä¢ Back-translation (translate ‚Üí translate back)
//! ```
//!
//! ### Time Series Augmentation
//!
//! ```
//! ‚Ä¢ Jittering (add noise)
//! ‚Ä¢ Scaling (multiply by constant)
//! ‚Ä¢ Time warping (speed up/slow down)
//! ‚Ä¢ Window slicing (different subsequences)
//! ‚Ä¢ Magnitude warping
//! ```
//!
//! **Benefits:**
//! ```
//! ‚úÖ Increases effective dataset size
//! ‚úÖ Improves generalization
//! ‚úÖ Reduces overfitting
//! ‚úÖ Makes model robust to variations
//! ‚úÖ Often better than more data
//! ```
//!
//! ## Combining Regularization Techniques
//!
//! ### Typical Configurations
//!
//! **Image Classification (ResNet-style):**
//! ```
//! ‚Ä¢ L2 weight decay: 0.0001
//! ‚Ä¢ Dropout: None (BatchNorm provides regularization)
//! ‚Ä¢ Data augmentation: Heavy (crop, flip, color jitter)
//! ‚Ä¢ Early stopping: Yes (patience=10)
//! ```
//!
//! **NLP (BERT-style):**
//! ```
//! ‚Ä¢ L2 weight decay: 0.01
//! ‚Ä¢ Dropout: 0.1 (after attention, embeddings)
//! ‚Ä¢ Data augmentation: Minimal
//! ‚Ä¢ Early stopping: Yes
//! ‚Ä¢ Label smoothing: 0.1
//! ```
//!
//! **Small Dataset:**
//! ```
//! ‚Ä¢ L2 weight decay: 0.001-0.01 (strong)
//! ‚Ä¢ Dropout: 0.5 (heavy)
//! ‚Ä¢ Data augmentation: Very heavy
//! ‚Ä¢ Early stopping: Yes (patience=20)
//! ```
//!
//! **Large Dataset:**
//! ```
//! ‚Ä¢ L2 weight decay: 0.0001 (light)
//! ‚Ä¢ Dropout: 0.0-0.2 (light or none)
//! ‚Ä¢ Data augmentation: Moderate
//! ‚Ä¢ Early stopping: Optional
//! ```
//!
//! ## Less Common Regularization Techniques
//!
//! ### Label Smoothing
//!
//! ```
//! Standard: Hard targets [0, 0, 1, 0]
//! Smoothed: Soft targets [0.05, 0.05, 0.85, 0.05]
//!
//! Formula:
//! y_smooth = (1-Œµ) √ó y + Œµ/K
//!
//! Where Œµ = 0.1 (typical), K = number of classes
//!
//! Benefits:
//! ‚Ä¢ Prevents overconfident predictions
//! ‚Ä¢ Better calibration
//! ‚Ä¢ Improves generalization
//! ```
//!
//! ### Stochastic Depth
//!
//! ```
//! Randomly drop entire layers during training
//!
//! For ResNet:
//! x = x + Block(x) with probability p
//! x = x with probability 1-p
//!
//! Benefits:
//! ‚Ä¢ Train deeper networks
//! ‚Ä¢ Implicit ensemble
//! ‚Ä¢ Used in very deep ResNets (100+)
//! ```
//!
//! ### Batch Normalization as Regularization
//!
//! ```
//! BatchNorm has regularization effect:
//! ‚Ä¢ Noise from batch statistics
//! ‚Ä¢ Acts like dropout
//! ‚Ä¢ Often sufficient for CNNs
//!
//! With BatchNorm:
//! ‚Ä¢ Can reduce/remove dropout
//! ‚Ä¢ Still use weight decay
//! ```
//!
//! ## Practical Tips
//!
//! ### How Much Regularization?
//!
//! ```
//! Too little:
//! ‚Ä¢ Training loss ‚Üí 0
//! ‚Ä¢ Validation loss stays high
//! ‚Ä¢ Large train-val gap
//!
//! Too much:
//! ‚Ä¢ Training loss stays high
//! ‚Ä¢ Model underfits
//! ‚Ä¢ Low capacity
//!
//! Just right:
//! ‚Ä¢ Training loss: Low but not zero
//! ‚Ä¢ Validation loss: Close to training
//! ‚Ä¢ Small train-val gap (< 5%)
//! ```
//!
//! ### Tuning Strategy
//!
//! ```
//! 1. Start without regularization
//!    ‚Üí Establish baseline, check for overfitting
//!
//! 2. Add weight decay (0.0001)
//!    ‚Üí Usually helps, minimal tuning needed
//!
//! 3. Add dropout if still overfitting (0.3-0.5)
//!    ‚Üí Start moderate, increase if needed
//!
//! 4. Add data augmentation
//!    ‚Üí Often biggest impact
//!
//! 5. Implement early stopping
//!    ‚Üí Always beneficial
//!
//! 6. Fine-tune hyperparameters
//!    ‚Üí Grid search or manual tuning
//! ```
//!
//! ### Common Mistakes
//!
//! ```
//! ‚ùå Using dropout at inference
//! ‚ùå Regularizing bias terms (usually not needed)
//! ‚ùå Same dropout rate for all layers
//! ‚ùå No validation set for early stopping
//! ‚ùå Too aggressive regularization
//! ‚ùå Forgetting to turn off dropout for eval()
//!
//! ‚úÖ Different dropout rates by layer type
//! ‚úÖ Monitor train vs validation gap
//! ‚úÖ Start with light regularization
//! ‚úÖ Always use validation set
//! ‚úÖ Combine multiple techniques
//! ```

fn main() {
    println!("=== Regularization & Dropout ===\n");

    println!("Techniques to prevent overfitting and improve generalization.\n");

    println!("üìö Techniques Covered:");
    println!("  ‚Ä¢ L1/L2 Regularization: Penalize large weights");
    println!("  ‚Ä¢ Dropout: Randomly drop neurons (0.3-0.5)");
    println!("  ‚Ä¢ Early Stopping: Stop when validation degrades");
    println!("  ‚Ä¢ Data Augmentation: Create more training data");
    println!("  ‚Ä¢ Label Smoothing: Prevent overconfidence\n");

    println!("üéØ Key Benefits:");
    println!("  ‚Ä¢ Prevents overfitting");
    println!("  ‚Ä¢ Improves test performance");
    println!("  ‚Ä¢ More robust models");
    println!("  ‚Ä¢ Better generalization\n");

    println!("üí° Typical Configuration:");
    println!("  ‚Ä¢ L2 weight decay: 0.0001-0.001");
    println!("  ‚Ä¢ Dropout: 0.3-0.5 (fully connected), 0.1-0.2 (CNNs)");
    println!("  ‚Ä¢ Early stopping: patience=10-20");
    println!("  ‚Ä¢ Data augmentation: Heavy for images\n");

    println!("See source code documentation for comprehensive explanations!");
}
