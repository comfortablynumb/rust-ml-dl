//! # Graph Neural Networks (GNN)
//!
//! This example explains Graph Neural Networks, designed for learning on
//! non-Euclidean data structures like social networks, molecules, and knowledge graphs.
//!
//! ## The Problem: Irregular Data
//!
//! **Standard neural networks assume grid structure:**
//!
//! ```
//! Images: Regular 2D grid
//! [pixel] [pixel] [pixel]
//! [pixel] [pixel] [pixel]
//! [pixel] [pixel] [pixel]
//! â†’ CNNs work great!
//!
//! Sequences: Regular 1D structure
//! [word] â†’ [word] â†’ [word] â†’ [word]
//! â†’ RNNs/Transformers work great!
//!
//! But what about:
//! Social networks: Irregular connections
//! Molecules: Variable number of atoms/bonds
//! Knowledge graphs: Arbitrary relationships
//! 3D meshes: Non-uniform structure
//! â†’ Need GNNs!
//! ```
//!
//! ## What is a Graph?
//!
//! ```
//! Graph G = (V, E)
//! â€¢ V: Set of nodes (vertices)
//! â€¢ E: Set of edges (connections)
//!
//! Example: Social Network
//! Nodes: People
//! Edges: Friendships
//!
//!     Alice ---- Bob
//!       |         |
//!     Carol ---- Dave
//!
//! Adjacency matrix A:
//! A[i,j] = 1 if edge between node i and j
//! A[i,j] = 0 otherwise
//!
//! Node features X:
//! X[i] = feature vector for node i
//! Example: [age, city, interests, ...]
//! ```
//!
//! ## Graph Tasks
//!
//! ### 1. Node Classification
//! ```
//! Predict label for each node
//!
//! Example: Social network
//! â€¢ Nodes: Users
//! â€¢ Task: Predict interests/communities
//! â€¢ Use: Friend connections help predict
//!
//! Citation network:
//! â€¢ Nodes: Papers
//! â€¢ Edges: Citations
//! â€¢ Task: Classify paper topic
//! ```
//!
//! ### 2. Link Prediction
//! ```
//! Predict missing or future edges
//!
//! Example: Social network
//! â€¢ Given: Current friendships
//! â€¢ Predict: Future friendships
//! â€¢ Use: Friend recommendations
//!
//! Drug interactions:
//! â€¢ Nodes: Drugs
//! â€¢ Predict: Which drugs interact
//! ```
//!
//! ### 3. Graph Classification
//! ```
//! Classify entire graph
//!
//! Example: Molecule classification
//! â€¢ Input: Molecular graph
//! â€¢ Output: Toxic or safe?
//! â€¢ Use: Drug discovery
//!
//! Protein function:
//! â€¢ Input: Protein structure graph
//! â€¢ Output: Function class
//! ```
//!
//! ## Message Passing: The Core Idea
//!
//! **Nodes communicate with neighbors:**
//!
//! ```
//! Goal: Learn node representations that incorporate neighborhood information
//!
//! Iteration:
//! 1. Each node receives messages from neighbors
//! 2. Aggregate messages
//! 3. Update own representation
//! 4. Repeat for L layers
//!
//! Result: L-hop neighborhood information
//! â€¢ Layer 1: Direct neighbors
//! â€¢ Layer 2: 2-hop neighbors
//! â€¢ Layer 3: 3-hop neighbors
//! ```
//!
//! ### Example: Friendship Influence
//!
//! ```
//! Task: Predict if user likes sports
//!
//! Initial features: h_i^(0) = user_features[i]
//!
//! Layer 1:
//! â€¢ Bob gets messages from Alice, Carol, Dave
//! â€¢ Aggregate: sum/mean/max of their features
//! â€¢ Update: h_Bob^(1) = f(h_Bob^(0), aggregate_neighbors)
//! â€¢ Now Bob's representation includes friend info!
//!
//! Layer 2:
//! â€¢ Bob gets messages from friends-of-friends
//! â€¢ Now h_Bob^(2) knows 2-hop neighborhood
//!
//! Final: Classify h_Bob^(L)
//! ```
//!
//! ## Graph Convolutional Network (GCN)
//!
//! **Most popular GNN architecture**
//!
//! ### Forward Pass
//!
//! ```
//! H^(l+1) = Ïƒ(DÌƒ^(-1/2) Ãƒ DÌƒ^(-1/2) H^(l) W^(l))
//!
//! Where:
//! â€¢ H^(l): Node features at layer l (N Ã— d matrix)
//! â€¢ A: Adjacency matrix
//! â€¢ Ãƒ = A + I (add self-loops)
//! â€¢ DÌƒ: Degree matrix of Ãƒ
//! â€¢ W^(l): Learnable weights
//! â€¢ Ïƒ: Activation (ReLU)
//!
//! Intuition:
//! 1. Multiply features by weights: H^(l) W^(l)
//! 2. Aggregate neighbors: Normalized adjacency Ã— features
//! 3. Activation: Ïƒ(...)
//!
//! Self-loops: Node includes its own features
//! Normalization: Prevent vanishing/exploding based on degree
//! ```
//!
//! ### Simplified Explanation
//!
//! ```
//! For each node i:
//!
//! h_i^(l+1) = Ïƒ(W^(l) Â· (Î£_jâˆˆN(i) h_j^(l) / âˆš(deg(i) Â· deg(j))))
//!
//! In words:
//! 1. Sum neighbor features: Î£ h_j
//! 2. Normalize by degree: / âˆš(deg(i) Â· deg(j))
//! 3. Apply linear transformation: W Â· ...
//! 4. Apply activation: Ïƒ(...)
//!
//! This is a "graph convolution"!
//! Similar to image convolution but for irregular graphs
//! ```
//!
//! ## GNN Variants
//!
//! ### GraphSAGE (Sample and Aggregate)
//!
//! ```
//! Problem: GCN uses all neighbors (expensive for large graphs)
//!
//! Solution: Sample fixed number of neighbors
//!
//! h_i^(l+1) = Ïƒ(W^(l) Â· [h_i^(l) || AGG({h_j^(l) : j âˆˆ S_i})])
//!
//! Where:
//! â€¢ S_i: Sampled neighbors of i (e.g., 10 neighbors)
//! â€¢ AGG: Mean, max, or LSTM aggregator
//! â€¢ ||: Concatenation
//!
//! Benefits:
//! â€¢ Scalable to large graphs
//! â€¢ Inductive (works on new nodes)
//! â€¢ Faster training
//! ```
//!
//! ### GAT (Graph Attention Network)
//!
//! ```
//! Idea: Learn importance of neighbors
//!
//! h_i^(l+1) = Ïƒ(Î£_jâˆˆN(i) Î±_ij W h_j^(l))
//!
//! Where Î±_ij: Attention weight for edge iâ†’j
//!
//! Attention mechanism:
//! e_ij = LeakyReLU(a^T [W h_i || W h_j])
//! Î±_ij = softmax_j(e_ij)
//!
//! Intuition:
//! â€¢ Learn which neighbors are more important
//! â€¢ Different weights for different edges
//! â€¢ Multi-head attention (like Transformers)
//!
//! Benefits:
//! â€¢ Adaptively weight neighbors
//! â€¢ Interpretable (attention scores)
//! â€¢ Often better performance
//! ```
//!
//! ### GIN (Graph Isomorphism Network)
//!
//! ```
//! Maximally powerful GNN (theoretically)
//!
//! h_i^(l+1) = MLP((1 + Îµ^(l)) Â· h_i^(l) + Î£_jâˆˆN(i) h_j^(l))
//!
//! Where:
//! â€¢ Îµ: Learnable parameter
//! â€¢ MLP: Multi-layer perceptron
//!
//! Key insight:
//! â€¢ Distinguishes more graph structures
//! â€¢ Provably as powerful as WL-test
//! â€¢ Good for graph classification
//! ```
//!
//! ## Pooling for Graph Classification
//!
//! **Aggregate node features to graph-level:**
//!
//! ### Global Pooling
//!
//! ```
//! Readout functions:
//!
//! 1. Global mean:
//! h_G = (1/N) Î£_i h_i
//!
//! 2. Global max:
//! h_G = max_i h_i (element-wise)
//!
//! 3. Global sum:
//! h_G = Î£_i h_i
//!
//! 4. Attention-based:
//! h_G = Î£_i Î±_i h_i  (learn Î±_i)
//! ```
//!
//! ### Hierarchical Pooling
//!
//! ```
//! Like max pooling in CNNs:
//!
//! 1. Cluster nodes into groups
//! 2. Create coarser graph
//! 3. Repeat
//!
//! Methods:
//! â€¢ DiffPool: Differentiable pooling
//! â€¢ TopKPool: Keep top-K nodes by score
//! â€¢ SAGPool: Self-attention pooling
//! ```
//!
//! ## Training GNNs
//!
//! ### Node Classification
//!
//! ```
//! # Semi-supervised learning
//! Only some nodes have labels
//!
//! Loss: Cross-entropy on labeled nodes
//! L = -Î£_{iâˆˆlabeled} y_i log(Å·_i)
//!
//! Training:
//! 1. Forward pass on entire graph
//! 2. Compute loss only on labeled nodes
//! 3. Backpropagate through message passing
//! 4. Update weights
//!
//! Inference: Predict all nodes (including unlabeled)
//! ```
//!
//! ### Graph Classification
//!
//! ```
//! Batch of graphs:
//! 1. Process each graph with GNN
//! 2. Pool to graph representation
//! 3. MLP classifier
//! 4. Cross-entropy loss
//!
//! Batch construction:
//! â€¢ Combine multiple graphs into one big graph
//! â€¢ Track graph membership
//! â€¢ Pool within each graph
//! ```
//!
//! ### Link Prediction
//!
//! ```
//! Learn edge existence:
//!
//! 1. Get node embeddings: h_i, h_j = GNN(graph)
//! 2. Compute edge score: score(i,j) = decoder(h_i, h_j)
//!
//! Decoders:
//! â€¢ Dot product: h_i^T h_j
//! â€¢ Concatenation + MLP: MLP([h_i || h_j])
//! â€¢ Distance: -||h_i - h_j||
//!
//! Loss: Binary cross-entropy
//! â€¢ Positive edges: Existing edges
//! â€¢ Negative edges: Random non-edges (sampling)
//! ```
//!
//! ## Applications
//!
//! ### Molecular Property Prediction
//!
//! ```
//! Graph representation:
//! â€¢ Nodes: Atoms (features: atomic number, charge, ...)
//! â€¢ Edges: Chemical bonds (features: bond type, ...)
//!
//! Tasks:
//! â€¢ Solubility prediction
//! â€¢ Toxicity classification
//! â€¢ Drug-target binding
//!
//! Impact:
//! â€¢ Accelerate drug discovery
//! â€¢ Predict properties without experiments
//! â€¢ ChEMBL dataset: 2M molecules
//! ```
//!
//! ### Social Network Analysis
//!
//! ```
//! Community detection:
//! â€¢ Nodes: Users
//! â€¢ Edges: Friendships
//! â€¢ Task: Cluster into communities
//!
//! Influence prediction:
//! â€¢ Who will adopt a product?
//! â€¢ Spread of information
//! â€¢ Recommendation systems
//!
//! Facebook Friend Suggestions:
//! â€¢ Link prediction
//! â€¢ Based on mutual friends, interests
//! ```
//!
//! ### Knowledge Graphs
//!
//! ```
//! Reasoning over facts:
//! â€¢ Nodes: Entities (people, places, concepts)
//! â€¢ Edges: Relations (is_a, born_in, works_at)
//!
//! Tasks:
//! â€¢ Link prediction: Complete missing facts
//! â€¢ Entity classification
//! â€¢ Question answering
//!
//! Examples:
//! â€¢ Google Knowledge Graph
//! â€¢ Wikidata
//! â€¢ BioMed knowledge bases
//! ```
//!
//! ### Recommendation Systems
//!
//! ```
//! Bipartite graph:
//! â€¢ Nodes: Users and Items
//! â€¢ Edges: User-item interactions
//!
//! Pinterest, YouTube, Amazon:
//! â€¢ GNN learns user/item embeddings
//! â€¢ Captures higher-order connectivity
//! â€¢ Better than matrix factorization
//!
//! PinSage (Pinterest):
//! â€¢ 3 billion pins
//! â€¢ GraphSAGE-based
//! â€¢ Production system
//! ```
//!
//! ### Traffic Prediction
//!
//! ```
//! Road network:
//! â€¢ Nodes: Road segments
//! â€¢ Edges: Connections
//! â€¢ Features: Speed, volume, time
//!
//! Spatio-temporal GNN:
//! â€¢ Spatial: Graph convolution
//! â€¢ Temporal: RNN/Transformer
//!
//! Uber, Google Maps:
//! â€¢ Predict traffic congestion
//! â€¢ Optimize routes
//! ```
//!
//! ### 3D Shape Analysis
//!
//! ```
//! 3D mesh as graph:
//! â€¢ Nodes: Vertices
//! â€¢ Edges: Mesh edges
//! â€¢ Features: 3D coordinates, normals
//!
//! Tasks:
//! â€¢ 3D shape classification
//! â€¢ Segmentation (part labeling)
//! â€¢ Shape generation
//!
//! Applications:
//! â€¢ 3D modeling
//! â€¢ Autonomous driving (LIDAR)
//! â€¢ Robotics
//! ```
//!
//! ## Challenges & Solutions
//!
//! ### Over-smoothing
//!
//! ```
//! Problem: Deep GNNs make all nodes similar
//! â€¢ After many layers, features converge
//! â€¢ Lose discriminative power
//!
//! Solutions:
//! â€¢ Shallow networks (2-3 layers often enough)
//! â€¢ Skip connections (like ResNet)
//! â€¢ Initial residual: h^(l+1) = h^(l) + GNN(h^(l))
//! â€¢ Jumping knowledge: Concatenate all layer outputs
//! ```
//!
//! ### Scalability
//!
//! ```
//! Problem: Large graphs (millions of nodes)
//!
//! Solutions:
//! 1. Sampling (GraphSAGE):
//!    â€¢ Sample neighbors instead of using all
//!    â€¢ Mini-batch training
//!
//! 2. Clustering:
//!    â€¢ Partition graph into clusters
//!    â€¢ Process clusters separately
//!
//! 3. Simplification:
//!    â€¢ Pre-compute propagation (SGC)
//!    â€¢ Linear models on pre-processed features
//! ```
//!
//! ### Heterogeneous Graphs
//!
//! ```
//! Multiple node/edge types:
//!
//! Example: Academic graph
//! â€¢ Nodes: Authors, Papers, Venues
//! â€¢ Edges: Writes, Cites, PublishedIn
//!
//! Solution: Heterogeneous GNN
//! â€¢ Different parameters per edge type
//! â€¢ Aggregate by relation type
//! â€¢ R-GCN, HGT (Heterogeneous Graph Transformer)
//! ```
//!
//! ### Dynamic Graphs
//!
//! ```
//! Graphs that change over time:
//!
//! Example: Social network
//! â€¢ New users join
//! â€¢ Friendships form/break
//! â€¢ User features change
//!
//! Solutions:
//! â€¢ Temporal GNN: GNN + RNN
//! â€¢ Snapshot-based: Process snapshots independently
//! â€¢ Continuous-time: Event-based updates
//! ```
//!
//! ## GNN vs Other Methods
//!
//! ### GNN vs Graph Kernels
//!
//! ```
//! Graph Kernels (traditional):
//! â€¢ Hand-crafted similarity functions
//! â€¢ No learned representations
//! â€¢ Limited expressiveness
//!
//! GNN:
//! âœ… Learn features end-to-end
//! âœ… More expressive
//! âœ… Better performance
//! âœ… Scalable
//! ```
//!
//! ### GNN vs Matrix Factorization
//!
//! ```
//! For link prediction/recommendation:
//!
//! Matrix Factorization:
//! â€¢ Only uses edge information
//! â€¢ Linear model
//!
//! GNN:
//! âœ… Uses node features
//! âœ… Higher-order connectivity
//! âœ… Non-linear
//! âœ… Better accuracy
//! ```
//!
//! ## Implementation Tips
//!
//! ### Number of Layers
//!
//! ```
//! 2-3 layers: Most tasks
//! â€¢ Sufficient for local neighborhoods
//! â€¢ Avoid over-smoothing
//!
//! 4-6 layers: Specific cases
//! â€¢ Need long-range dependencies
//! â€¢ Use skip connections
//!
//! > 6 layers: Rarely helps
//! â€¢ Over-smoothing issue
//! â€¢ Use graph transformers instead
//! ```
//!
//! ### Hyperparameters
//!
//! ```
//! Hidden dimensions: 64-512
//! Learning rate: 0.001-0.01
//! Dropout: 0.5 (prevents overfitting)
//! Batch size: 32-128 (graph classification)
//! Optimizer: Adam
//! Epochs: 100-500
//! ```
//!
//! ### Data Splits
//!
//! ```
//! Node classification:
//! â€¢ Transductive: Train on partial graph
//! â€¢ Inductive: Train on separate graphs
//!
//! Graph classification:
//! â€¢ Standard train/val/test split
//! â€¢ Stratify by class
//! ```
//!
//! ## Modern Developments
//!
//! ### Graph Transformers
//!
//! ```
//! Apply transformer attention to graphs:
//! â€¢ Attention over all nodes (not just neighbors)
//! â€¢ Positional encodings for graph structure
//! â€¢ More expressive than GNNs
//!
//! Examples:
//! â€¢ Graph Transformer (GT)
//! â€¢ GraphGPS
//! â€¢ Graphormer
//!
//! Trade-off: O(NÂ²) complexity
//! ```
//!
//! ### Graph Foundation Models
//!
//! ```
//! Pre-train on large graph datasets:
//! â€¢ Self-supervised learning
//! â€¢ Transfer to downstream tasks
//!
//! Examples:
//! â€¢ GraphMAE (masked autoencoders)
//! â€¢ GraphCL (contrastive learning)
//! â€¢ GROVER (molecular pre-training)
//! ```
//!
//! ## Historical Impact
//!
//! **2009:** Spectral graph convolutions
//! - Theoretical foundation
//! - Not practical
//!
//! **2014:** DeepWalk, Node2Vec
//! - Graph embeddings
//! - Random walk based
//!
//! **2016:** GCN (Graph Convolutional Network)
//! - Kipf & Welling
//! - Practical message passing
//! - Breakthrough paper
//!
//! **2017:** GraphSAGE
//! - Sampling for scalability
//! - Inductive learning
//!
//! **2018:** GAT (Graph Attention)
//! - Attention mechanism
//! - Better performance
//!
//! **2019:** GIN (Graph Isomorphism)
//! - Theoretical expressiveness
//! - WL-test equivalence
//!
//! **2020+:** Widespread adoption
//! - Pinterest (PinSage)
//! - Alibaba (recommendations)
//! - DeepMind (AlphaFold uses GNN)
//! - Drug discovery companies
//!
//! **Legacy:**
//! - Enabled learning on graph-structured data
//! - Key component in modern AI systems
//! - Active research area

fn main() {
    println!("=== Graph Neural Networks (GNN) ===\n");

    println!("This example explains GNNs, neural networks for graph-structured data");
    println!("like social networks, molecules, and knowledge graphs.\n");

    println!("ðŸ“š Key Concepts Covered:");
    println!("  â€¢ Graph representation and tasks");
    println!("  â€¢ Message passing framework");
    println!("  â€¢ GCN, GraphSAGE, GAT architectures");
    println!("  â€¢ Node, edge, and graph-level predictions");
    println!("  â€¢ Applications: molecules, social networks, recommendations");
    println!("  â€¢ Scalability and over-smoothing challenges\n");

    println!("ðŸŽ¯ Why This Matters:");
    println!("  â€¢ Handles non-Euclidean data (not grids or sequences)");
    println!("  â€¢ Powers modern recommendation systems (Pinterest, YouTube)");
    println!("  â€¢ Accelerates drug discovery (molecular property prediction)");
    println!("  â€¢ Enables knowledge graph reasoning");
    println!("  â€¢ Used in AlphaFold, traffic prediction, 3D analysis\n");

    println!("See the source code documentation for comprehensive explanations!");
}
