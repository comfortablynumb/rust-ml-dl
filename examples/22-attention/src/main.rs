//! # Attention Mechanisms: The Foundation of Modern AI
//!
//! This example explains attention mechanisms, the core innovation that powers
//! Transformers, BERT, GPT, and most modern AI systems.
//!
//! ## The Core Problem: Context Awareness
//!
//! **Traditional sequence processing:**
//! ```
//! RNN/LSTM: Process sequentially, limited context
//! "The cat sat on the mat" ‚Üí h_final
//!                              ‚Üë
//!           Single vector must capture everything!
//!
//! Problem:
//! ‚Ä¢ Information bottleneck
//! ‚Ä¢ Difficult for long sequences
//! ‚Ä¢ Can't focus on relevant parts
//! ```
//!
//! **Solution: Attention**
//! ```
//! "The cat sat on the mat"
//!   ‚Üì   ‚Üì   ‚Üì   ‚Üì   ‚Üì   ‚Üì
//! When translating "sat" ‚Üí Look at all words, focus on relevant ones
//! Attention weights: [0.05, 0.30, 0.50, 0.10, 0.03, 0.02]
//!                           ‚Üë    ‚Üë
//!                         "cat" "sat" most relevant!
//! ```
//!
//! ## Intuition: Human Attention
//!
//! ```
//! Reading: "The quick brown fox jumps over the lazy dog"
//! Question: "What color is the fox?"
//!
//! Human behavior:
//! ‚Ä¢ Don't re-read entire sentence
//! ‚Ä¢ Focus attention on "brown fox"
//! ‚Ä¢ Ignore irrelevant words
//!
//! Neural attention: Same idea!
//! ‚Ä¢ Compute relevance scores for all words
//! ‚Ä¢ Focus on important parts
//! ‚Ä¢ Weighted combination
//! ```
//!
//! ## Attention Formula
//!
//! **Basic attention:**
//! ```
//! Attention(Q, K, V) = softmax(score(Q, K)) ¬∑ V
//!
//! Where:
//! ‚Ä¢ Q: Query ("what am I looking for?")
//! ‚Ä¢ K: Keys ("what do I have?")
//! ‚Ä¢ V: Values ("what information to return?")
//! ‚Ä¢ score: Similarity function
//!
//! Steps:
//! 1. Compute scores: How relevant is each position?
//! 2. Softmax: Convert to probabilities (weights sum to 1)
//! 3. Weighted sum: Combine values by relevance
//! ```
//!
//! ## Attention Variants
//!
//! ### 1. Additive Attention (Bahdanau, 2014)
//!
//! ```
//! score(h_i, s_j) = v^T tanh(W_1 h_i + W_2 s_j)
//!
//! Where:
//! ‚Ä¢ h_i: Encoder hidden state
//! ‚Ä¢ s_j: Decoder hidden state
//! ‚Ä¢ W_1, W_2, v: Learnable parameters
//!
//! Used in: First attention-based machine translation
//! ```
//!
//! ### 2. Multiplicative Attention (Luong, 2015)
//!
//! ```
//! score(h_i, s_j) = h_i^T W s_j
//!
//! Simpler, faster than additive
//! Fewer parameters
//! ```
//!
//! ### 3. Scaled Dot-Product Attention (Vaswani, 2017)
//!
//! **The attention used in Transformers:**
//!
//! ```
//! Attention(Q, K, V) = softmax(QK^T / ‚àöd_k) V
//!
//! Where:
//! ‚Ä¢ Q: Query matrix (n √ó d_k)
//! ‚Ä¢ K: Key matrix (m √ó d_k)
//! ‚Ä¢ V: Value matrix (m √ó d_v)
//! ‚Ä¢ d_k: Dimension of keys
//! ‚Ä¢ ‚àöd_k: Scaling factor
//!
//! Why scale by ‚àöd_k?
//! ‚Ä¢ Dot products grow large for high dimensions
//! ‚Ä¢ Large values ‚Üí softmax saturation
//! ‚Ä¢ Scaling keeps gradients healthy
//!
//! Matrix form:
//! Input: n queries, m key-value pairs
//! Scores: QK^T ‚Üí (n √ó m) matrix
//! Weights: softmax ‚Üí (n √ó m) probabilities
//! Output: Weights √ó V ‚Üí (n √ó d_v)
//! ```
//!
//! ### Example Calculation
//!
//! ```
//! Sentence: "I love cats"
//! Embedding dim: 4
//!
//! Q = [[1,0,1,0]]  ‚Üê Query for "love"
//! K = [[1,0,0,1],  ‚Üê Keys for "I"
//!      [1,0,1,0],  ‚Üê "love"
//!      [0,1,1,1]]  ‚Üê "cats"
//! V = [[0.2,0.1,0.3,0.4],  ‚Üê Values for "I"
//!      [0.5,0.5,0.0,0.0],  ‚Üê "love"
//!      [0.1,0.9,0.0,0.0]]  ‚Üê "cats"
//!
//! Step 1: Compute scores
//! QK^T = [1,0,1,0] ¬∑ [[1,0,0,1]^T, [1,0,1,0]^T, [0,1,1,1]^T]
//!      = [1, 2, 1]  ‚Üê Raw scores
//!
//! Step 2: Scale
//! Scaled = [1, 2, 1] / ‚àö4 = [0.5, 1.0, 0.5]
//!
//! Step 3: Softmax
//! Weights = softmax([0.5, 1.0, 0.5])
//!         = [0.21, 0.58, 0.21]  ‚Üê Attention weights!
//!
//! Step 4: Weighted sum of values
//! Output = 0.21¬∑V[0] + 0.58¬∑V[1] + 0.21¬∑V[2]
//!        = [0.35, 0.42, 0.06, 0.08]
//!
//! Interpretation: "love" attends mostly to itself (0.58)
//! ```
//!
//! ## Self-Attention
//!
//! **Attend to the same sequence:**
//!
//! ```
//! Regular attention: Encoder ‚Üí Decoder
//! Self-attention: Sequence attends to itself
//!
//! "The animal didn't cross the street because it was too tired"
//!                                                    ‚Üë
//!                                            What is "it"?
//!
//! Self-attention weights for "it":
//! [0.02, 0.62, 0.05, 0.03, 0.01, 0.05, 0.01, 0.01, 0.15, 0.05]
//!        ‚Üë                                                ‚Üë
//!    "animal" (0.62)                               "tired" (0.15)
//!
//! ‚Üí "it" = animal!
//! ```
//!
//! ### Computing Self-Attention
//!
//! ```
//! Input: Sequence of embeddings X (n √ó d)
//!
//! Create Q, K, V from same input:
//! Q = X W_Q  ‚Üê Query projection
//! K = X W_K  ‚Üê Key projection
//! V = X W_V  ‚Üê Value projection
//!
//! Attention(Q, K, V) = softmax(QK^T / ‚àöd_k) V
//!
//! Each position attends to all positions!
//! ```
//!
//! ## Multi-Head Attention
//!
//! **Attend to different aspects simultaneously:**
//!
//! ```
//! Single head: One attention pattern
//! Multi-head: Multiple parallel attention patterns
//!
//! head_i = Attention(Q W_Q^i, K W_K^i, V W_V^i)
//!
//! Example with 8 heads:
//! Head 1: Syntactic relationships (subject-verb)
//! Head 2: Semantic relationships (synonyms)
//! Head 3: Positional patterns (nearby words)
//! Head 4: Long-range dependencies
//! ...and so on
//!
//! Concatenate all heads:
//! MultiHead(Q,K,V) = Concat(head_1, ..., head_h) W_O
//!
//! Benefits:
//! ‚Ä¢ Capture different types of relationships
//! ‚Ä¢ More robust
//! ‚Ä¢ Richer representations
//! ```
//!
//! ## Masked Attention
//!
//! **Prevent looking ahead (for autoregressive models):**
//!
//! ```
//! Problem in language generation:
//! Generating "The cat sat on the"
//! Should not see "mat" yet!
//!
//! Solution: Mask future positions
//!
//! Scores before masking:
//! [[s11, s12, s13, s14],
//!  [s21, s22, s23, s24],
//!  [s31, s32, s33, s34],
//!  [s41, s42, s43, s44]]
//!
//! Apply mask (set future to -‚àû):
//! [[s11, -‚àû,  -‚àû,  -‚àû ],
//!  [s21, s22, -‚àû,  -‚àû ],
//!  [s31, s32, s33, -‚àû ],
//!  [s41, s42, s43, s44]]
//!
//! After softmax, -‚àû ‚Üí 0:
//! [[1.00, 0.00, 0.00, 0.00],  ‚Üê Position 1 only sees itself
//!  [0.30, 0.70, 0.00, 0.00],  ‚Üê Position 2 sees 1,2
//!  [0.10, 0.25, 0.65, 0.00],  ‚Üê Position 3 sees 1,2,3
//!  [0.05, 0.15, 0.30, 0.50]]  ‚Üê Position 4 sees all
//!
//! Used in: GPT, decoder-only models
//! ```
//!
//! ## Cross-Attention
//!
//! **Attend to a different sequence:**
//!
//! ```
//! Encoder-Decoder attention:
//! ‚Ä¢ Q: From decoder (target sequence)
//! ‚Ä¢ K, V: From encoder (source sequence)
//!
//! Translation example:
//! Source: "I love cats"  ‚Üí Encoder ‚Üí K, V
//! Target: "J'aime les"   ‚Üí Decoder ‚Üí Q
//!
//! Decoder attends to source!
//! Aligns target words with source words
//!
//! Used in: Translation, image captioning, text-to-image
//! ```
//!
//! ## Applications
//!
//! ### 1. Machine Translation (Seq2Seq + Attention)
//!
//! ```
//! Without attention:
//! Source ‚Üí Encoder ‚Üí Fixed vector ‚Üí Decoder ‚Üí Target
//!                       ‚Üë
//!              Information bottleneck!
//!
//! With attention:
//! Source ‚Üí Encoder ‚Üí All hidden states
//!                         ‚Üì
//!                    Decoder attends to relevant parts
//!                         ‚Üì
//!                      Target
//!
//! BLEU score improvement: +5-10 points!
//! ```
//!
//! ### 2. Image Captioning
//!
//! ```
//! CNN ‚Üí Image features (grid: 7√ó7√ó512)
//!         ‚Üì
//! Decoder generates caption word by word
//! Each word attends to different image regions
//!
//! "A dog" ‚Üí Attend to dog region
//! "playing" ‚Üí Attend to action area
//! "with ball" ‚Üí Attend to ball
//!
//! Interpretable! Can visualize where model looks
//! ```
//!
//! ### 3. Document Classification
//!
//! ```
//! Self-attention over document:
//! ‚Ä¢ Find important sentences
//! ‚Ä¢ Context-aware representations
//! ‚Ä¢ Better than averaging
//!
//! Hierarchical attention:
//! ‚Ä¢ Word-level attention (within sentences)
//! ‚Ä¢ Sentence-level attention (within document)
//! ```
//!
//! ### 4. Question Answering
//!
//! ```
//! Context: Paragraph of text
//! Question: "What is...?"
//!
//! Cross-attention:
//! ‚Ä¢ Question attends to context
//! ‚Ä¢ Find relevant spans
//! ‚Ä¢ Extract answer
//!
//! Used in: BERT, RoBERTa for SQuAD
//! ```
//!
//! ## Attention Visualization
//!
//! **Interpreting what the model learned:**
//!
//! ```
//! Attention weights = how much each position matters
//!
//! Heatmap visualization:
//!             I    love  cats
//! I        [0.5   0.3   0.2]
//! love     [0.2   0.4   0.4]  ‚Üê "love" attends to "cats"
//! cats     [0.1   0.2   0.7]  ‚Üê "cats" attends to itself
//!
//! Patterns reveal:
//! ‚Ä¢ Syntactic structure (subject-verb-object)
//! ‚Ä¢ Semantic relationships (related concepts)
//! ‚Ä¢ Coreference (pronouns to nouns)
//! ```
//!
//! ## Computational Complexity
//!
//! ```
//! Self-attention complexity:
//!
//! Time: O(n¬≤ ¬∑ d)
//! ‚Ä¢ n: Sequence length
//! ‚Ä¢ d: Dimension
//! ‚Ä¢ n¬≤ from comparing all pairs
//!
//! Memory: O(n¬≤)
//! ‚Ä¢ Store attention matrix
//!
//! Problem for long sequences:
//! ‚Ä¢ n=512: 262K entries
//! ‚Ä¢ n=1024: 1M entries
//! ‚Ä¢ n=4096: 16M entries
//!
//! Solutions:
//! ‚Ä¢ Sparse attention (Longformer)
//! ‚Ä¢ Linear attention (Linformer)
//! ‚Ä¢ Local attention windows
//! ‚Ä¢ Compressed attention (Reformer)
//! ```
//!
//! ## Implementation Considerations
//!
//! ### Efficient Matrix Operations
//!
//! ```
//! Batch matrix multiplication:
//! ‚Ä¢ Process multiple sequences at once
//! ‚Ä¢ GPU-friendly operations
//! ‚Ä¢ Parallelizable
//!
//! Typical shapes:
//! Q: (batch, heads, seq_len, d_k)
//! K: (batch, heads, seq_len, d_k)
//! V: (batch, heads, seq_len, d_v)
//!
//! Scores: (batch, heads, seq_len, seq_len)
//! Output: (batch, heads, seq_len, d_v)
//! ```
//!
//! ### Dropout in Attention
//!
//! ```
//! Apply dropout to attention weights:
//! weights = softmax(scores)
//! weights = dropout(weights, p=0.1)
//! output = weights ¬∑ V
//!
//! Benefits:
//! ‚Ä¢ Regularization
//! ‚Ä¢ Prevents over-reliance on specific positions
//! ‚Ä¢ Better generalization
//! ```
//!
//! ## Historical Impact
//!
//! **2014:** Bahdanau attention (machine translation)
//! - First successful attention mechanism
//! - Beat pure seq2seq models
//!
//! **2015:** Luong attention variations
//! - Simpler, more efficient
//! - Global vs local attention
//!
//! **2017:** Transformer ("Attention is All You Need")
//! - Self-attention only, no RNNs
//! - Multi-head attention
//! - Foundation of modern NLP
//!
//! **2018-2020:** Attention everywhere
//! - BERT: Bidirectional attention
//! - GPT: Masked attention
//! - Vision: Attention in CNNs
//!
//! **2021+:** Transformers dominate
//! - NLP: Almost all models use attention
//! - Vision: ViT, DETR
//! - Multi-modal: CLIP, DALL-E
//! - Foundation models: GPT-4, PaLM
//!
//! ## Why Attention Won
//!
//! ```
//! vs RNN/LSTM:
//! ‚úÖ Parallelizable (no sequential dependency)
//! ‚úÖ Better long-range dependencies
//! ‚úÖ No vanishing gradients through layers
//! ‚úÖ Interpretable (visualize attention)
//!
//! vs CNN:
//! ‚úÖ Global receptive field (see entire input)
//! ‚úÖ Position-independent
//! ‚úÖ Dynamic weights (data-dependent)
//!
//! Trade-off:
//! ‚ùå O(n¬≤) complexity
//! ‚ùå Need positional information (add encodings)
//! ```

fn main() {
    println!("=== Attention Mechanisms: Foundation of Modern AI ===\n");

    println!("This example explains attention mechanisms, the core innovation");
    println!("powering Transformers, GPT, BERT, and most modern AI.\n");

    println!("üìö Key Concepts Covered:");
    println!("  ‚Ä¢ Query, Key, Value framework");
    println!("  ‚Ä¢ Scaled dot-product attention");
    println!("  ‚Ä¢ Self-attention vs cross-attention");
    println!("  ‚Ä¢ Multi-head attention");
    println!("  ‚Ä¢ Masked attention for autoregressive models");
    println!("  ‚Ä¢ Attention visualization and interpretability\n");

    println!("üéØ Why This Matters:");
    println!("  ‚Ä¢ Foundation of Transformers (GPT, BERT, T5)");
    println!("  ‚Ä¢ Replaced RNNs as primary sequence model");
    println!("  ‚Ä¢ Enables parallelization and long-range dependencies");
    println!("  ‚Ä¢ Powers all modern NLP, vision transformers, multi-modal AI");
    println!("  ‚Ä¢ Most important ML innovation of the 2010s\n");

    println!("See the source code documentation for comprehensive explanations!");
}
